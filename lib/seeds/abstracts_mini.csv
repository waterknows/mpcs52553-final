description,name
"We consider a problem motivated by the desire to provide flexible, rate-based, quality of service guarantees for packets sent over input queued switches and switch networks. Our focus is solving a type of online traffic scheduling problem, whose input at each time step is a set of desired traffic rates through the switch network. These traffic rates in general cannot be exactly achieved since they assume arbitrarily small fractions of packets can be transmitted at each time step. The goal of the traffic scheduling problem is to closely approximate the given sequence of traffic rates by a sequence of transmissions in which only whole packets are sent. We prove worst-case bounds on the additional buffer use, which we call backlog, that results from using such an approximation.We first consider the N Ã— N, input queued, crossbar switch. Our main result is an online packet-scheduling algorithm using no speedup that guarantees backlog at most (N+1)2/4 packets at each input port and each output port. Upper bounds on worst-case backlog have been proved for the case of constant fluid schedules, such as the N2-2N+2 bound of Chang, Chen, and Huang (INFOCOM, 2000). Our main result for the crossbar switch is the first, to our knowledge, to bound backlog in terms of switch size N for arbitrary, time-varying fluid schedules, without using speedup.Our main result for Banyan networks is an exact characterization of the speedup required to maintain bounded backlog, in terms of polytopes derived from the network topology.",Approximating fluid schedules in crossbar packet-switches and Banyan networks
"The multi-stage, multi-machine capacitated lot-sizing problem (MSMMCLSP) consists of scheduling the production of one product in a multi-machine production system with a multi-stage structure. Machine capacity is reserved for the production of the end item but can be extended by working overtime (overtime capacity). When a lot size is positive in a specific period, it can be loaded on all machines without exceeding the sum of the regular and overtime capacity limits. In previous research [Rong, C., Takahashi, K., & Morikawa, K. (2005). A new heuristic method for capacitated lot-sizing in multi-stage, multi-machine production systems. Journal of Japan Industrial Management Association, 56(5), in press], we have proposed a rescheduling heuristic to determine a feasible and sufficient solution for the MSMMCLSP with only regular capacity limits, in comparison with Franca et al.'s heuristic method. However, the mechanism of rescheduling in regular time has not optimum (or near optimum) performance for the MSMMCLSP with capacity extension. Preliminary examination revealed that overtime production also has situation-dependent advantages and disadvantages with the combination of various planning parameters. This study, therefore, develops an integrated heuristic that guarantees an optimal solution for general MSMMCLSP by combining the rescheduling mechanism and overtime production. We also evaluate the effectiveness of the proposed heuristics by using computational tests with various planning parameters.",MRP Rescheduling heuristics with capacity extension under deterministic demand
"This paper focuses on the problem of how data representation influences the generalization error of kernel based learning machines like support vector machines (SVM) for classification. Frame theory provides a well founded mathematical framework for representing data in many different ways. We analyze the effects of sparse and dense data representations on the generalization error of such learning machines measured by using leave-one-out error given a finite amount of training data. We show that, in the case of sparse data representations, the generalization error of an SVM trained by using polynomial or Gaussian kernel functions is equal to the one of a linear SVM. This is equivalent to saying that the capacity of separating points of functions belonging to hypothesis spaces induced by polynomial or Gaussian kernel functions reduces to the capacity of a separating hyperplane in the input space. Moreover, we show that, in general, sparse data representations increase or leave unchanged the generalization error of kernel based methods. Dense data representations, on the contrary, reduce the generalization error in the case of very large frames. We use two different schemes for representing data in overcomplete systems of Haar and Gabor functions, and measure SVM generalization error on benchmarked data sets.",Data representations and generalization error in kernel based learning machines
"The hit-or-miss transform (HMT) is a fundamental operation on binary images, widely used since 40 years. As it is not increasing, its extension to grey-level images is not straightforward, and very few authors have considered it. Moreover, despite its potential usefulness, very few applications of the grey-level HMT have been proposed until now. Part I of this paper, developed hereafter, is devoted to the description of a theory leading to a unification of the main definitions of the grey-level HMT, mainly proposed by Ronse and Soille, respectively (part II will deal with the applicative potential of the grey-level HMT, which will be illustrated by its use for vessel segmentation from 3D angiographic data). In this first part, we review the previous approaches to the grey-level HMT, especially the supremal one of Ronse, and the integral one of Soille; the latter was defined only for flat structuring elements (SEs), but it can be generalized to non-flat ones. We present a unified theory of the grey-level HMT, which is decomposed into two steps. First a fitting associates to each point the set of grey-levels for which the SEs can be fitted to the image; as in Soille's approach, this fitting step can be constrained. Next, a valuation associates a final grey-level value to each point; we propose three valuations: supremal (as in Ronse), integral (as in Soille) and binary.",Grey-level hit-or-miss transforms-Part I: Unified theory
"Since their introduction as a means of front propagation and their first application to edge-based segmentation in the early 90's, level set methods have become increasingly popular as a general framework for image segmentation. In this paper, we present a survey of a specific class of region-based level set segmentation methods and clarify how they can all be derived from a common statistical framework.Region-based segmentation schemes aim at partitioning the image domain by progressively fitting statistical models to the intensity, color, texture or motion in each of a set of regions. In contrast to edge-based schemes such as the classical Snakes, region-based methods tend to be less sensitive to noise. For typical images, the respective cost functionals tend to have less local minima which makes them particularly well-suited for local optimization methods such as the level set method.We detail a general statistical formulation for level set segmentation. Subsequently, we clarify how the integration of various low level criteria leads to a set of cost functionals. We point out relations between the different segmentation schemes. In experimental results, we demonstrate how the level set function is driven to partition the image plane into domains of coherent color, texture, dynamic texture or motion. Moreover, the Bayesian formulation allows to introduce prior shape knowledge into the level set method. We briefly review a number of advances in this domain.","A Review of Statistical Approaches to Level Set Segmentation: Integrating Color, Texture, Motion and Shape"
"We address the pose mismatch problem which can occur in face verification systems that have only a single (frontal) face image available for training. In the framework of a Bayesian classifier based on mixtures of gaussians, the problem is tackled through extending each frontal face model with artificially synthesized models for non-frontal views. The synthesis methods are based on several implementations of maximum likelihood linear regression (MLLR), as well as standard multi-variate linear regression (LinReg). All synthesis techniques rely on prior information and learn how face models for the frontal view are related to face models for non-frontal views. The synthesis and extension approach is evaluated by applying it to two face verification systems: a holistic system (based on PCA-derived features) and a local feature system (based on DCT-derived features). Experiments on the FERET database suggest that for the holistic system, the LinReg-based technique is more suited than the MLLR-based techniques; for the local feature system, the results show that synthesis via a new MLLR implementation obtains better performance than synthesis based on traditional MLLR. The results further suggest that extending frontal models considerably reduces errors. It is also shown that the local feature system is less affected by view changes than the holistic system; this can be attributed to the parts based representation of the face, and, due to the classifier based on mixtures of gaussians, the lack of constraints on spatial relations between the face parts, allowing for deformations and movements of face areas.",On transforming statistical models for non-frontal face verification
"In this paper, we propose a convolution filtering scheme for detecting small defects in low-contrast uniform surface images and, especially, focus on the applications for backlight panels and glass substrates found in liquid crystal display (LCD) manufacturing. A defect embedded in a low-contrast surface image shows no distinct intensity from its surrounding region, and even worse, the sensed image may present uneven brightness on the surface. All these make the defect detection in low-contrast surface images extremely difficult. In this study, a constrained independent component analysis (ICA) model is proposed to design an optimal filter with the objective that the convolution filter will generate the most representative source intensity of the background surface without noise. The prior constraint incorporated in the ICA model confines the source values of all training image patches of a defect-free image within a small interval of control limits. In the inspection process, the same control parameter used in the constraint is also applied to set up the thresholds that make impulse responses of all pixels in faultless regions within the control limits, and those in defective regions outside the control limits. A stochastic evolutionary computation algorithm, particle swarm optimization (PSO), is applied to solve for the constrained ICA model. Experimental results have shown that the proposed method can effectively detect small defects in low-contrast backlight panels and LCD glass substrate images.",An independent component analysis-based filter design for defect detection in low-contrast surface images
"We present an efficient algorithm for approximating huge general volumetric data sets, i.e. the data is given over arbitrarily shaped volumes and consists of up to millions of samples. The method is based on cubic trivariate splines, i.e. piecewise polynomials of total degree three defined w.r.t, uniform type-6 tetrahedral partitions of the volumetric domain. Similar as in the recent bivariate approximation approaches (cf. [10, 15]), the splines in three variables are automatically determined from the discrete data as a result of a two-step method (see [40]), where local discrete least squares polynomial approximations of varying degrees are extended by using natural conditions, i.e. the continuity and smoothness properties which determine the underlying spline space. The main advantages of this approach with linear algorithmic complexity are as follows: no tetrahedral partition of the volume data is needed, only small linear systems have to be solved, the local variation and distribution of the data is automatically adapted, Bernstein-B&eacute;zier techniques well-known in Computer Aided Geometric Design (CAGD) can be fully exploited, noisy data are automatically smoothed. Our numerical examples with huge data sets for synthetic data as well as some real-world data confirm the efficiency of the methods, show the high quality of the spline approximation, and illustrate that the rendered iso-surfaces inherit a visual smooth appearance from the volume approximating splines.",Spline approximation of general volumetric data
"In this paper, we propose a novel variational method for color image segmentation using modified geodesic active contour method. Our goal is to detect Object(s) of Interest (OOI) from a given color image, regardless of other objects. The main novelty of our method is that we modify the stopping function in the functional of usual geodesic active contour method so that the new stopping function is coupled by a discrimination function of OOI. By minimizing the functional, the OOI is segmented. Firstly, we study the pixel properties of the OOI by sample pixels visually chosen from OOI. From these sample pixels, by the principal component analysis and interval estimation, the discrimination function of whether a pixel is in the OOI is obtained probabilistically. Then we propose the energy functional for the segmentation of OOI with new stopping function. Unlike usual stopping functions defined by the image gradient, our improved stopping function depends on not only the image gradient but also the discrimination function derived from the color information of OOI. As a result, better than usual active contour methods which detect all objects in the image, our modified active contour method can detect OOI but without unwanted objects. Experiments are conducted in both synthetic and natural images. The result shows that our algorithm is very efficient for detecting OOI even the background is complicated.",Color Image Segmentation for Objects of Interest with Modified Geodesic Active Contour Method
"Automatic creation of B-rep models of engineering objects from freehand sketches would benefit designers. A subgoal is to take a single line drawing (with hidden lines removed), and from it deduce an initial 3D geometric realisation of the visible part of the object. Junction and line labels, and provisional depth coordinates, are important components of this frontal geometry. Most methods for producing frontal geometry use line labelling, but this takes little or no account of geometry. As a result, the line labels produced can be unreliable.Previously, we proposed an approach which inflates a drawing to produce provisional depth coordinates, and uses these to make deductions about line labels. Even a na&iuml;ve implementation can outperform previous line labelling methods in certain cases. In this paper, we further enhance this approach. We extend the algorithm to non-isometric-projection drawings, consider improved ways of realising some of the concepts, and also consider how to combine this approach with other labelling techniques to gain the benefits of each.We test our approach using to be drawings of what we consider representative samples of engineering objects; these exemplify difficulties not considered in many previous papers on line labelling. Our results, based on this test set, show that the enhancements result in significant benefits.",Making the most of using depth reasoning to label line drawings of engineering objects
"We present a novel approach for interactively synthesizing motions for characters navigating in complex environments. We focus on the runtime efficiency for motion generation, thereby enabling the interactive animation of a large number of characters simultaneously. The key idea is to precompute search trees of motion clips that can be applied to arbitrary environments. Given a navigation goal relative to a current body position, the best available solution paths and motion sequences can be efficiently extracted during runtime through a series of table lookups. For distant start and goal positions, we first use a fast coarse-level planner to generate a rough path of intermediate sub-goals to guide each iteration of the runtime lookup phase.We demonstrate the efficiency of our technique across a range of examples in an interactive application with multiple autonomous characters navigating in dynamic environments. Each character responds in real-time to arbitrary user changes to the environment obstacles or navigation goals. The runtime phase is more than two orders of magnitude faster than existing planning methods or traditional motion synthesis techniques. Our technique is not only useful for autonomous motion generation in games, virtual reality, and interactive simulations, but also for animating massive crowds of characters offline for special effects in movies.",Precomputed search trees: planning for interactive goal-driven animation
"Given a sequence of n real numbers and an integer k, 1 â‰¤ k â‰¤ 1/2n(n - 1), the k maximum-sum segments problem is to locate the k segments whose sums are the k largest among all possible segment sums. Recently, Bengtsson and Chen gave an O(min{k + n log2 n, nâˆšk})-time algorithm for this problem. Bae and Takaoka later proposed a more efficient algorithm for small k. In this paper, we propose an O(n + k log(min{n, k}))-time algorithm for the same problem, which is superior to both of them when k is o(n log n). We also give the first optimal algorithm for delivering the k maximum-sum segments in non-decreasing order if k â‰¤ n. Then we develop an O(n2d-1 + k log min{n, k})-time algorithm for the d-dimensional version of the problem, where d > 1 and each dimension, without loss of generality, is of the same size n. This improves the best previously known O(n2d-1C)-time algorithm, also by Bengtsson and Chen, where C = min{k + n log2 n, nâˆšk}. It should be pointed out that, given a two-dimensional array of size m Ã— n, our algorithm for finding the k maximum-sum subarrays is the first one achieving cubic time provided that k is O(m2n/log n).",Improved algorithmms for the k maximum-sums problems
"Use different real positive numbers p""i to represent all kinds of pattern categories, after mapping the inputted patterns into a special feature space by a non-linear mapping, a linear relation between the mapped patterns and numbers p""i is assumed, whose bias and coefficients are undetermined, and the hyper-plane corresponding to zero output of the linear relation is looked as the base hyper-plane. To determine the pending parameters, an objective function is founded aiming to minimize the difference between the outputs of the patterns belonging to a same type and the corresponding p""i, and to maximize the distance between any two different hyper-planes corresponding to different pattern types. The objective function is same to that of support vector regression in form, so the coefficients and bias of the linear relation are calculated by some known methods such as SVM^l^i^g^h^t approach. Simultaneously, three methods are also given to determine p""i, the best one is to determine them in training process, which has relatively high accuracy. Experiment results of the IRIS data set show that, the accuracy of this method is better than those of many SVM-based multi-class classifiers, and close to that of DAGSVM (decision-directed acyclic graph SVM), emphatically, the recognition speed is the highest.",A novel and quick SVM-based multi-class classifier
"Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.",Collective entity resolution in relational data
"The complexity of animating trees, shrubs and foliage is an impediment to the efficient and realistic depiction of natural environments. This paper presents an algorithm to extract, from a single video sequence, motion fields of real shrubs under the influence of wind, and to transfer this motion to the animation of complex, synthetic 3D plant models. The extracted motion is retargeted without requiring physical simulation. First, feature tracking is applied to the video footage, allowing the 2D position and velocity of automatically identified features to be clustered. A key contribution of the method is that the hierarchy obtained through statistical clustering can be used to synthesize a 2D hierarchical geometric structure of branches that terminates according to the cut-off threshold of a classification algorithm. This step extracts both the shape and the motion of a hierarchy of features groups that are identified as geometrical branches. The 2D hierarchy is then extended to three dimensions using the estimated spatial distribution of the features within each group. Another key contribution is that this 3D hierarchical structure can be efficiently used as a motion controller to animate any complex 3D model of similar but non-identical plants using a standard skinning algorithm. Thus, a single video source of a moving shrub becomes an input device for a large class of virtual shrubs. We illustrate the results on two examples of shrubs and one outdoor tree. Extensions to other outdoor plants are discussed.",Hierarchical retargetting of 2D motion fields to the animation of 3D plant models
"We consider involutory antimorphisms Ï• of a free monoid A* and their fixed points, called Ï•-palindromes or pseudopalindromes. A Ï•-palindrome reduces to a usual palindrome when Ï• is the reversal operator. For any word w âˆˆ A* the right (resp. left) Ï•-palindrome closure of w is the shortest Ï•-palindrome having w as a prefix (resp. suffix). We prove some results relating Ï•-palindrome closure operators with periodicity and conjugacy, and derive some interesting closure properties for the languages of finite Sturmian and episturmian words. In particular, a finite word w is Sturmian if and only if both its palindromic closures are so. Moreover, in such a case, both the palindromic closures of w share the same minimal period of w. A new characterization of finite Sturmian words follows, in terms of periodicity and special factors of their palindromic closures. Some weaker results can be extended to the episturmian case. By using the right Ï•-palindrome closure, we extend the construction of standard episturmian words via directive words. In this way one obtains a family of infinite words, called Ï•-standard words, which are morphic images of episturmian words, as well as a wider family of infinite words including the Thue-Morse word on two symbols.",Pseudopalindrome closure operators in free monoids
"This paper proposes a hybrid genetic algorithm (a-hGA) with adaptive local search scheme. For designing the a-hGA, a local search technique is incorporated in the loop of genetic algorithm (GA), and whether or not the local search technique is used in the GA is automatically determined by the adaptive local search scheme. Two modes of adaptive local search schemes are developed in this paper. First mode is to use the conditional local search method that can measure the average fitness values obtained from the continuous two generations of the a-hGA, while second one is to apply the similarity coefficient method that can measure a similarity among the individuals of the population of the a-hGA. These two adaptive local search schemes are included in the a-hGA loop, respectively. Therefore, the a-hGA can be divided into two types: a- hGA1 and a-hGA2. To prove the efficiency of the a-hGA1 and a-hGA2, a canonical GA (cGA) and a hybrid GA (hGA) with local search technique and without any adaptive local search scheme are also presented. In numerical example, all the algorithms (cGA, hGA, a-hGA1 and a-hGA2) are tested and analyzed. Finally, the efficiency of the proposed a-hGA1 and a-hGA2 is proved by various measures of performance.",Hybrid genetic algorithm with adaptive local search scheme
"Graph bandwidth minimization (GBM) is a classical and challenging problem in graph algorithms and combinatorial optimization. Most of existing researches on this problem have focused on unweighted graphs. In this paper, we study the bandwidth minimization problem of weighted caterpillars, and propose several algorithms for solving various types of caterpillars and general graphs. More specifically, we show that the GBM problem on caterpillars with hair-length at most 2 and the GBM problem on star-shape caterpillars are NP-complete, and give a lower bound of the graph bandwidth for general weighted graphs. For caterpillars with hair-length at most 1, we present an O(n log n log(nwmax))-time algorithm to compute an optimal bandwidth layout, where n is the total number of vertices in the graph and wmax is the maximum wedge weight. For caterpillars with hair-length at most k, we give a k-approximation algorithm. For arbitrary caterpillars and general graphs, we give a heuristic algorithm and some experimental results. Experiments show that the solutions obtained by our heuristic algorithm are roughly within a factor of clog(n) of the lower bound for a small number c, which is consistent with the inapproximability results of this problem (i.e., no constant approximation for the GBM problem unless P = NP).",Graph bandwidth of weighted caterpillars
"One of the typical causes of errors in team cooperative activities, such as in central control rooms of power plants and cockpits in aircrafts, is conflicts among team members' intentions. If mutual awareness and communication were perfectly established and maintained, conflicts could be detected and recovered by team members; however, this does not happen in practice. In this paper, we provide a framework for detecting conflicts among team members' intentions based on team intention inference, aiming to make machines function as a coordinator for cooperative activities. In previous work, we developed a method for team intention inference based on a definition of 'we-intention'. We-intention is other-regarding intentions relating to situations in which some agents act together, and is represented as a set of individual intentions and mutual beliefs. In this framework, a conflict can be defined as a set of individual intentions and false beliefs (undesired procedures), and detected by searching for such combinations. We applied the proposed method to the operation of a plant simulator operated by a two-person team, and it was confirmed through an experiment that this method could list candidates for conflicts by type and set the actual conflict high in priority in the tested context.",A method for conflict detection based on team intention inference
"We present a method for scattered data approximation with subdivision surfaces which actually uses the true representation of the limit surface as a linear combination of smooth basis functions associated with the control vertices. This is unlike previous techniques which used only piecewise linear approximations of the limit surface. By this we can assign arbitrary parameterizations to the given sample points, including those generated by parameter correction. We present a robust and fast algorithm for exact closest point search on Loop surfaces by combining Newton iteration and non-linear minimization. Based on this we perform unconditionally convergent parameter correction to optimize the approximation with respect to the L2 metric and thus we make a well-established scattered data fitting technique which has been available before only for B-spline surfaces, applicable to subdivision surfaces. Further we exploit the fact that the control mesh of a subdivision surface can have arbitrary connectivity to reduce the L&infin; error up to a certain user-defined tolerance by adaptively restructuring the control mesh. By employing iterative least squares solvers, we achieve acceptable running times even for large amounts of data and we obtain high quality approximations by surfaces with relatively low control mesh complexity compared to the number of sample points. Since we are using plain subdivision surfaces, there is no need for multiresolution detail coefficients and we do not have to deal with the additional overhead in data and computational complexity associated with them.",Optimization techniques for approximation with subdivision surfaces
"K42 is one of the few recent research projects that is examining operating system design structure issues in the context of new whole-system design. K42 is open source and was designed from the ground up to perform well and to be scalable, customizable, and maintainable. The project was begun in 1996 by a team at IBM Research. Over the last nine years there has been a development effort on K42 from between six to twenty researchers and developers across IBM, collaborating universities, and national laboratories. K42 supports the Linux API and ABI, and is able to run unmodified Linux applications and libraries. The approach we took in K42 to achieve scalability and customizability has been successful.The project has produced positive research results, has resulted in contributions to Linux and the Xen hypervisor on Power, and continues to be a rich platform for exploring system software technology. Today, K42, is one of the key exploratory platforms in the DOE's FAST-OS program, is being used as a prototyping vehicle in IBM's PERCS project, and is being used by universities and national labs for exploratory research. In this paper, we provide insight into building an entire system by discussing the motivation and history of K42, describing its fundamental technologies, and presenting an overview of the research directions we have been pursuing.",K42: building a complete operating system
"Diffusions of new products and technologies through social networks can be formalized as spreading of infectious diseases. However, while epidemiological models describe infection in terms of transmissibility, we propose a diffusion model that explicitly includes consumer decision-making affected by social influences and word-of-mouth processes. In our agent-based model consumers' probability of adoption depends on the external marketing effort and on the internal influence that each consumer perceives in his/her personal networks. Maintaining a given marketing effort and assuming its effect on the probability of adoption as linear, we can study how social processes affect diffusion dynamics and how the speed of the diffusion depends on the network structure and on consumer heterogeneity. First, we show that the speed of diffusion changes with the degree of randomness in the network. In markets with high social influence and in which consumers have a sufficiently large local network, the speed is low in regular networks, it increases in small-world networks and, contrarily to what epidemic models suggest, it becomes very low again in random networks. Second, we show that heterogeneity helps the diffusion. Ceteris paribus and varying the degree of heterogeneity in the population of agents simulation results show that the more heterogeneous the population, the faster the speed of the diffusion. These results can contribute to the development of marketing strategies for the launch and the dissemination of new products and technologies, especially in turbulent and fashionable markets.",Diffusion dynamics in small-world networks with heterogeneous consumers
"Energy efficiency becomes increasingly important in today's high-performance storage systems. It can be challenging to save energy and improve performance at the same time in conventional (i.e. single-rotation-rate) disk-based storage systems. Most existing solutions compromise performance for energy conservation. In this paper, we propose a redundancy-based, two-level I/O cache architecture called RIMAC to address this problem. The idea of RIMAC is to enable data on the standby disk to be recovered by accessing data in the two-level I/O cache or on currently active/idle disks. At both cache and disk levels, RIMAC dynamically transforms accesses toward standby disks by exploiting parity redundancy in parity-based redundant disk arrays. Because I/O requests that require physical accesses on standby disks involve long waiting time and high power consumption for disk spin-up (tens of seconds for SCSI disks), transforming those requests to accesses in a two-level, collaborative I/O cache or on active disks can significantly improve both energy efficiency and performance.In RIMAC, we developed i) two power-aware read request transformation schemes called Transformable Read in Cache (TRC) and Transformable Read on Disk (TRD), ii) a power-aware write request transformation policy for parity update and iii) a second-chance parity cache replacement algorithm to improve request transformation rate. We evaluated RIMAC by augmenting a validated storage system simulator, disksim. For several real-life server traces including HP's cello 99, TPC-D and SPC's search engine, RIMAC is shown to reduce energy consumption by up to 33% and simultaneously improve the average response time by up to 30%.","RIMAC: a novel redundancy-based hierarchical cache architecture for energy efficient, high performance storage systems"
"Signature-based network intrusion-detection systems (NIDSs) often report a massive number of simple alerts of low-level security-related events. Many of these alerts are logically involved in a single multi-stage intrusion incident and a security officer often wants to analyze the complete incident instead of each individual simple alert. This paper proposes a well-structured model that abstracts the logical relation between the alerts in order to support automatic correlation of those alerts involved in the same intrusion. The basic building block of the model is a logical formula called a capability. We use capability to abstract consistently and precisely all levels of accesses obtained by the attacker in each step of a multistage intrusion. We then derive inference rules to define logical relations between different capabilities. Based on the model and the inference rules, we have developed several novel alert correlation algorithms and implemented a prototype alert correlator. The experimental results of the correlator using several intrusion datasets demonstrate that the approach is effective in both alert fusion and alert correlation and has the ability to correlate alerts of complex multistage intrusions. In several instances, the alert correlator successfully correlated more than two thousand Snort alerts involved in massive scanning incidents. It also helped us find two multistage intrusions that were missed in auditing by the security officers.",Modeling network intrusion detection alerts for correlation
"In the literature, on the numerical solution of nonlinear time dependent partial differential equations, much attention has been paid to numerical processes which have the favourable property of being total variation bounded (TVB). A popular approach to guaranteeing the TVB property consists in demanding that the process has the stronger property of being total variation diminishing (TVD).For Runge-Kutta methods--applied to semi-discrete approximations of partial differential equations--conditions on the time step were established which guarantee the TVD property; see, e.g., [J. Comput. Phys. 77 (1988) 439; Math. Comp. 67 (1998) 73; SIAM Rev. 43 (2001) 89; SIAM J. Numer. Anal. (2002), in press; Higueras, Tech. Report, Universidad PÃºblica de Navarra, 2002; SIAM J. Numer. Anal. 40 (2002) 469]. These conditions were derived under the assumption that the simple explicit Euler time stepping process is TVD.However, for various important semi-discrete approximations, the Euler process is TVB but not TVD--see, e.g., [Math. Comp. 49 (1987) 105; Math. Comp. 52 (1989) 411]. Accordingly, the above stepsize conditions for Runge-Kutta methods are not directly relevant to such approximations, and there is a need for stepsize restrictions with a wider range of applications.In this paper, we propose a general theory yielding stepsize restrictions which cover a larger class of semi-discrete approximations than covered thus far in the literature. In particular, our theory gives stepsize restrictions, for general Runge-Kutta methods, which guarantee total-variation-boundedness in situations where the Euler process is TVB but not TVD.",Stepsize restrictions for total-variation-boundedness in general Runge-Kutta procedures
"When conducting experiments, the selected quality characteristic should as far as possible be a continuous variable and be easy to measure. Due to the inherent nature of the quality characteristic or the convenience of the measurement technique and cost-effectiveness, the data observed in many experiments are ordered categorical. To analyze ordered categorical data for optimizing factor settings, there are three widely accepted approaches: Taguchi's accumulation analysis, Nair's scoring scheme and Jeng's weighted probability scoring scheme. In this paper, a simpler method named the weighted SN ratio method for analyzing ordered categorical data is introduced. A case study involving optimizing the polysilicon deposition process for minimizing surface defects and achieving the target thickness in a very large-scale integrated circuit can demonstrate the four approaches. Finally, comparative analyses of efficiency for employing the four approaches to optimize factor settings are presented according to simulated experimental data that are normally, Weibull and Gamma distributed. From the results, it is obvious that the weighted SN ratio method has the properties of easy computation and uses one-step optimization to obtain the optimal factor settings. Its efficiency is slightly less than that of the scoring scheme, better than that of the accumulation analysis and the weighted probability-scoring scheme.",A comparative study on optimization methods for experiments with ordered categorical data
"This paper presents a new technique of computer-aided analysis and recognition of pathological wrist bone lesions. This method uses artificial intelligence (AI) techniques and mathematical linguistics allowing to evaluate automatically and analyse the structure of the said bones, based on palm radiological images. Possibilities of computer interpretation of selected images, based on the methodology of automatic medical image understanding, as introduced by the authors, were created owing to the introduction of an original relational description of individual palm (wrist) bones. This description has been built with the use of graph linguistic formalisms already applied in artificial intelligence. These were, however, developed and adjusted to the needs of automatic medical image understanding in earlier works of the authors, as specified in the bibliography section of this paper. The research described in this paper has demonstrated that the for needs of palm (wrist) bone diagnostics, specialist linguistic tools such as expansive graph grammars and EDT-label graphs are particularly well-suited. Defining a graph image language adjusted to the specific features of the scientific problem here-described allowed for a semantic description of correct palm bone structures (with consideration to idiosyncratic features). It also enabled interpretation of images showing some in-born lesions, such as additional bones; or acquired lesions such as their incorrect junctions resulting from injuries and synostoses.",Image languages in intelligent radiological palm diagnostics
"In parallel computing structures, Hypercubes [P. J. Wan, L. W. Liu, Y. Yang, Optimal routing based on the super-topology in Hypercube WDM networks, 1999, pp. 142-149] and [Y. R. Leu, S. Y. Kuo, A fault-tolerant tree communication scheme for hypercube systems, IEEE Trans Comput. 45(6) (1996) 643-650] have many advantages: they support parallel computing, provide disjoint paths, and tolerate faults. If devices with computing capabilities can be linked as a Hypercube by taking advantage of Bluetooth radio's features, then an efficient communication and high-performance computing environment can be established by applying currently used algorithms. A Bluetooth device randomly searches for and connects with other devices, using time-consuming inquiry/inquiry scan and page/page scan operation and hence, results in an uncontrolled scatternet topology and inefficient communications. The present work proposes a three-stage distributed construction protocol for rapidly organizing a Hypercube computing environment that was constructed from Bluetooth devices. The proposed protocol governs the construction of links, the assigning of roles and the formation of the scatternet in order to efficiently construct a Hypercube structure. The constructed scatternet easily enables Bluetooth devices to establish a routing path, tolerate faults and create disjoint paths, and thus, achieves parallel and distributed computing in a Bluetooth wireless environment. Experimental results reveal that the proposed protocol can set up a scatternet that is appropriate for parallel computing and communications.",BlueCube: constructing a hypercube parallel computing and communication environment over bluetooth radio systems
"Parameterization of 3D meshes is important for many graphics and CAD applications, in particular for texture mapping, re-meshing and morphing. Current parameterization methods for closed manifold genus-n meshes usually involve cutting the mesh according to the object generators, fixing the resulting boundary and then applying the 2D position for each of the mesh vertices on a plane, such that the flattened triangles are not too distorted and do not overlap. Unfortunately, fixing the boundary distorts the resulting parameterization, especially near the boundary. A special case is that of closed manifold genus-1 meshes that have two generators. They can therefore be flattened naturally to a plane without the use of a fixed boundary while still maintaining the continuity of the parameterization. Therefore, in treating genus-1 objects, this attribute must be exploited. This paper introduces a generalized method for planar parameterization of closed manifold genus-1 meshes. As in any planar parameterization with a fixed boundary, weights are assigned over the mesh edges. The type of weights defined depends on the type of mesh characteristics to be preserved. The paper proves that the method satisfies the non-overlapping requirement for any type of positive barycentric weights, including nonsymmetrical weights. Moreover, convergence is guaranteed according to the Gauss-Seidel method. The proposed method is simple to implement, fast and robust. The feasibility of the method will be demonstrated on several complex objects.",Planar parameterization for closed 2-manifold genus-1 meshes
"Wireless devices now hold multiple radio interfaces, allowing to switch from one network to another according to required connectivity and related quality. Still, the selection of the best radio interface for a specific connection is under the responsibility of the end-user in most cases. Integrated multi-radio network management so as to improve the overall performance of the network(s) up to the software application layer, has led to a number of research efforts over the last few years. However, several challenges remain due to the inherent complexity of the problem. This paper specifically concentrates on the comprehensive analysis of energy-efficient multi-radio networking for pervasive computing. Building upon the service oriented architectural style, we consider pervasive networks of software services, which are deployed on the various networked nodes. The issue is then to optimize the energetic performance of the pervasive network through careful selection of the radio link over which service access should be realized for each such access. By considering the most common wireless interfaces in use today (Bluetooth, WiFi and GPRS), we introduce a formal model of service-oriented multi-radio networks. The proposed model enables characterizing the optimal network configuration in terms of energetic performance, which is shown to be a NP-hard problem and thus requires adequate approximation.",Energetic performance of service-oriented multi-radio networks: issues and perspectives
"In [7], a new information&dash;theoretic attribute selection method for decision tree induction was introduced. This method consists in computing for each node, a distance between the partition generated by the values of each candidate attribute in the node and the correct partition of the subset of training examples in this node. The chosen attribute is that whose corresponding partition is the closest to the correct partition (i.e., the partition that perfectly classifies the training data). In that paper it was also formally proved that such distance is not biased towards attributes with a large number of values in the sense specified by Quinlan in [12] and some initial experimental evidence suggests that the predictive accuracy of the induced trees was not significantly different from that obtained with the most widely used information theoretic attribute selection measures, that is, Quinlan&rsquo;s Gain and Quinlan&rsquo;s Gain Ratio. However, it seemed that the distance induced smaller trees especially when the attributes had different number of values. In that paper it was not confirmed that the differences were statistically significant due to the small number of experiments performed. In this paper we report experimental results that allow to confirm that the distance induces trees whose size, without losing accuracy, is not significantly different from those obtained using Quinlan&rsquo;s Gain but smaller than those obtained with Quinlan&rsquo;s Gain Ratio. These experimental results are supported by a statistical analysis performed using two statistical hypothesis tests&colon; the sign test and the signed rank test.",Comparing information&dash;theoretic attribute selection measures&colon; a statistical approach
"It has been shown that as long as traffic sources adapt their rates to aggregate congestion measure in their paths, they implicitly maximize certain utility. In this paper we study some counter-intuitive throughput behaviors in such networks, pertaining to whether a fair allocation is always inefficient and whether increasing capacity always raises aggregate throughput. A bandwidth allocation policy can be defined in terms of a class of utility functions parameterized by a scalar Î± that can be interpreted as a quantitative measure of fairness. An allocation is fair if Î± is large and efficient if aggregate throughput is large. All examples in the literature suggest that a fair allocation is necessarily inefficient. We characterize exactly the tradeoff between fairness and throughput in general networks. The characterization allows us both to produce the first counter-example and trivially explain all the previous supporting examples. Surprisingly, our counter-example has the property that a fairer allocation is always more efficient. In particular it implies that maxmin fairness can achieve a higher throughput than proportional fairness. Intuitively, we might expect that increasing link capacities always raises aggregate throughput. We show that not only can throughput be reduced when some link increases its capacity, more strikingly, it can also be reduced when all links increase their capacities by the same amount. If all links increase their capacities proportionally, however, throughput will indeed increase. These examples demonstrate the intricate interactions among sources in a network setting that are missing in a single-link topology.",Counter-intuitive throughput behaviors in networks under end-to-end control
"This work studies three variants of a three-machine flowshop problem with two operations per job to minimize make-span (F3/o = 2/Cmax). A set of n jobs are classified into three mutually exclusive families A,B and C. The familiesA,B and C are defined as the set of jobs that is scheduled in machine sequence (M1, M2), (M1, M3) and (M1, M3), respectively, where (Mx, My) specifies the machine sequence for the job that is processed first on Mx, and then on My. Specifically, jobs with the same route (machine sequence) are classified into the same family. Three variants of F3/o = 2/Cmax are studied. First, F3/ GT, no-idle, o = 2/Cmax, in which both machine no-idle and GT restrictions are considered. The GT assumption requires that all jobs in the same family are processed contiguously on the machine and the machine no-idle assumption requires that all machines work continuously without idle time. Second, the problem F3/GT, o = 2/Cmax, in which the machine no-idle restriction in the first variant is relaxed, is considered. Third, the problem F3/no-idle, o = 2/Cmax with the GT assumption in the first variant relaxed is considered. Based on the dominance conditions developed, the optimal solution is polynomially derived for each variant. These results may narrow down the gap between easy and hard cases of the general problem.",Three-machine flowshop with two operations per job to minimize makespan
"Restoring data operations after a disaster is a daunting task: how should recovery be performed to minimize data loss and application downtime? Administrators are under considerable pressure to recover quickly, so they lack time to make good scheduling decisions. They schedule recovery based on rules of thumb, or on pre-determined orders that might not be best for the failure occurrence. With multiple workloads and recovery techniques, the number of possibilities is large, so the decision process is not trivial.This paper makes several contributions to the area of data recovery scheduling. First, we formalize the description of potential recovery processes by defining recovery graphs. Recovery graphs explicitly capture alternative approaches for recovering workloads, including their recovery tasks, operational states, timing information and precedence relationships. Second, we formulate the data recovery scheduling problem as an optimization problem, where the goal is to find the schedule that minimizes the financial penalties due to downtime, data loss and vulnerability to subsequent failures. Third, we present several methods for finding optimal or near-optimal solutions, including priority-based, randomized and genetic algorithm-guided ad hoc heuristics. We quantitatively evaluate these methods using realistic storage system designs and workloads, and compare the quality of the algorithms' solutions to optimal solutions provided by a math programming formulation and to the solutions from a simple heuristic that emulates the choices made by human administrators. We find that our heuristics' solutions improve on the administrator heuristic's solutions, often approaching or achieving optimality.",On the road to recovery: restoring data after disasters
"Nearly ten years after its first presentation and five years after its first application to operating systems, the suitability of Aspect-Oriented Programming (AOP) for the development of operating system kernels is still highly in dispute. While the AOP advocacy emphasizes the benefits of AOP towards better configurability and maintainability of system software, most kernel developers express a sound skepticism regarding the thereby induced runtime and memory costs: Operating system kernels have to be lean and efficient.We have analyzed the runtime and memory costs of aspects in general, on the level of &mu;-benchmarks, and by refactoring and extending the eCos operating system kernel using AspectC++, an AOP extension to the C++ language. Our results show that most AOP features do not induce a intrinsic overhead and that the actual overhead induced by AspectC++ is very low. We have also analyzed a test case with significant aspect-related costs. This example shows how the structure of the underlying kernel can have a negative impact on aspect implementations and how these costs can be avoided by an aspect-aware design.Based on this analysis, our conclusion is that AOP is suitable for the development of operating system kernels and other kinds of highly efficient infrastructure software.",A quantitative analysis of aspects in the eCos kernel
"In this paper we present an empirical study of a workload gathered by crawling the eDonkey network --- a dominant peer-to-peer file sharing system --- for over 50 days.We first confirm the presence of some known features, in particular the prevalence of free-riding and the Zipf-like distribution of file popularity. We also analyze the evolution of document popularity.We then provide an in-depth analysis of several clustering properties of such workloads. We measure the geographical clustering of peers offering a given file. We find that most files are offered mostly by peers of a single country, although popular files don't have such a clear home country.We then analyze the overlap between contents offered by different peers. We find that peer contents are highly clustered according to several metrics of interest.We propose to leverage this property by allowing peers to search for content without server support, by querying suitably identified semantic neighbours. We find via trace-driven simulations that this approach is generally effective, and is even more effective for rare files. If we further allow peers to query both their semantic neighbours, and in turn their neighbours' neighbours, we attain hit rates as high as over 55% for neighbour lists of size 20.","Peer sharing behaviour in the eDonkey network, and implications for the design of server-less file sharing systems"
"Most of the work on behavior prediction in the field of Qualitative Reasoning has focused on transient behavior and responses to perturbations&semi; very little has been done regarding systems in steady state. A large class of systems, especially in the area of power systems, are designed for sinusoidal steady&dash;state operation. Thus, an understanding of the steady state behavior of electrical circuits is very important.This article presents a framework for reasoning about linear electrical circuits in sinusoidal steady state. The reasoning process relies on a constraint&dash;based model of the circuit, derived from electro&dash;magnetic theory and generated automatically from the structure of the circuit. In a linear circuit operating in steady state, all quantities are sinusoidals of the same frequency as the source. Since any sinusoidal can be expressed as the real part of a complex exponential, we use the complex form, which simplifies computations&semi; this complex form, characterized by magnitude and angle, is called a phasor. In order to capture magnitude and phase angle information in the model, all constraints operate on phasor variables.Constraint Propagation (CP) is the main inference mechanism. The CP module reasons with as much information and precision as the user provides, ranging from qualitative to quantitative. Intervals provide a general representation mechanism.The framework presented in this article has been implemented in a program called Qualitative Phasor Analysis (QPA), which performs circuit analysis, parameter design, diagnosis, control design, and structure simplification. Circuits with multiple sources are solved using the superposition principle.",Reasoning about linear circuits; a model-based approach
"Scatter search is a population-based method that has recently been shown to yield promising outcomes for solving combinatorial and nonlinear optimization problems. Based on formulations originally proposed in 1960s for combining decision rules and problem constraints such as the surrogate constraint method, scatter search uses strategies for combining solution vectors that have proved effective in a variety of problem settings. In this paper, we develop a general purpose heuristic for a class of nonlinear optimization problems. The procedure is based on the scatter search methodology and treats the objective function evaluation as a black box, making the search algorithm context-independent. Most optimization problems in the chemical and bio-chemical industries are highly nonlinear in either the objective function or the constraints. Moreover, they usually present differential-algebraic systems of constraints. In this type of problem, the evaluation of a solution or even the feasibility test of a set of values for the decision variables is a time-consuming operation. In this context, the solution method is limited to a reduced number of solution examinations. We have implemented a scatter search procedure in Matlab (Mathworks, 2004) for this special class of difficult optimization problems. Our development goes beyond a simple exercise of applying scatter search to this class of problems, but presents innovative mechanisms to obtain a good balance between intensification and diversification in a short-term search horizon. Computational comparisons with other recent methods over a set of benchmark problems favor the proposed procedure.",Scatter search for chemical and bio-process optimization
"Reflection on software engineering as a sub-discipline within computer science reveals that it is as much about people and teamwork as it is about technical expertise. It is therefore important that a software engineer is not only competent in software development but also able to work effectively in a team. Based on the scholarly literature we derived a simple model for team composition and used it to guide, without coercion, our students during team formation. This research reports on an investigation into the role of personality diversity within teams of tertiary students taking a course in software engineering. Data collected confirms that both personality diversity in teams as well as competence of teams impacts positively on team performance. In contrast to other studies, our research data does not seek to couple personality with appropriate role allocation in the team; rather, our measurement is in respect of ""raw"" team diversity alone. In this study the correlation between personality diversity and the success of teams were captured at different stages during the development of a software engineering project tackled by the teams. The same was done with correlation between competence and team success. The personality diversity of the teams showed a strong correlation with team success during the inception phase. This correlation however weakened during the course of the year while the correlation between competence and success started slightly weaker than personality diversity during the inception phase, but grew very strong towards the completion of the team projects.",Software engineering team diversity and performance
"cJ is an extension of Java that allows supertypes, fields, and methods of a class or interface to be provided only under some static subtyping condition. For instance, a cJ generic class, C<P>, may provide a member method m only when the type provided for parameter P is a subtype of a specific type Q.From a practical standpoint, cJ adds to generic Java classes and interfaces the ability to express case-specific code. Unlike conditional compilation techniques (e.g., the C/C++ ""#ifdef"" construct) cJ is statically type safe and maintains the modular type-checking properties of Java generic classes: a cJ generic class can be checked independently of the code that uses it. Just like regular Java, checking a cJ class implies that all uses are safe, under the contract for type parameters specified in the class's signature.As a specific application, cJ addresses the well-known shortcomings of the Java Collections Framework (JCF). JCF data structures often throw run-time errors when an ""optional"" method is called upon an object that does not support it. Within the constraints of standard Java, the authors of the JCF had to either sacrifice static type safety or suffer a combinatorial explosion of the number of types involved. cJ avoids both problems, maintaining both static safety and conciseness.",cJ: enhancing java with safe type conditions
"Stochastic optimization problems attempt to model uncertainty in the data by assuming that the input is specified by a probability distribution. We consider the well-studied paradigm of 2-stage models with recourse: first, given only distributional information about (some of) the data one commits on initial actions, and then once the actual data is realized (according to the distribution), further (recourse) actions can be taken. We show that for a broad class of 2-stage linear models with recourse, one can, for any &epsi; > 0, in time polynomial in 1/&epsi; and the size of the input, compute a solution of value within a factor (1&plus;&epsi;) of the optimum, in spite of the fact that exponentially many second-stage scenarios may occur. In conjunction with a suitable rounding scheme, this yields the first approximation algorithms for 2-stage stochastic integer optimization problems where the underlying random data is given by a &ldquo;black box&rdquo; and no restrictions are placed on the costs in the two stages. Our rounding approach for stochastic integer programs shows that an approximation algorithm for a deterministic analogue yields, with a small constant-factor loss, provably near-optimal solutions for the stochastic generalization. Among the range of applications, we consider are stochastic versions of the multicommodity flow, set cover, vertex cover, and facility location problems.",An approximation scheme for stochastic linear programming and its application to stochastic integer programs
"In a network of bufferless packet multiplexers, the user-perceived capacity of an ingress-egress tunnel (connection) may degrade quickly with increasing path length. This is due to the compounding of transmission blocking probabilities along the path of the connection, even when the links are not overloaded. In such an environment, providing users (e.g., client ISPs) with tunnels of statistically guaranteed bandwidth may limit the network's connection-carrying capacity.In this paper, we introduce and analyze a transmission-scheduling algorithm that employs randomization and traffic regulation at the ingress, and batch scheduling at the links. The algorithm ensures that a fraction of transmissions from each connection is consistently subject to small blocking probability at every link, so that these transmissions are likely to survive long paths. For this algorithm, we obtain tight bounds on the expectation and tail probability of the blocking rate of any ingress-egress connection. We compare the bounds to those obtained using the FCFS link-scheduling rule. We find that the proposed scheduling algorithm significantly improves the network's connection-carrying capacity.In deriving the desired bounds, we develop an analytic framework for stochastically comparing network-wide routing and bandwidth allocation scenarios with respect to blocking in a packet multiplexer. The framework enables us to formally characterize the routing and bandwidth allocation scenarios that maximize the expected blocking rate along the path of a tagged connection.",Analysis of a transmission scheduling algorithm for supporting bandwidth guarantees in bufferless networks
"This work presents numerical assessments of possible routing paths for the reported late Pleistocene Missoula floods, which involved Earth's largest known peak discharges of freshwater. For our numerical model, we adopt the diffusion wave approximation of the shallow-water equation with the empirical Manning coefficient. This simplification is required because (1) most previous studies are one-dimensional or analytical studies based on simple equations, so that our two-dimensional expansion is a natural step towards more sophisticated models; (2) a simple model can easily incorporate the effect of complicated topography; and (3) lower computational costs permit a broader exploration of parameters. Important and new insights obtained from this work include: (1) numerically, we confirm the presence of the Cordilleran Ice Sheet at the time of the Missoula floods; (2) floods with relatively small peak discharges cannot inundate some major reaches, including the Cheney Palouse Scabland Tract; (3) deposits indicative of multiple floods are mostly confined in the locations that can be inundated by relatively lower peak flood discharges; and (4) although Lake Missoula glacial lake failure scenario can reasonably reproduce extensive water coverage, the best fit result to the geological evidence of flooding is obtained when the total water volume is three times larger than that estimated for Glacial Lake Missoula. The above results do not negate previous works, but indicate the need for further detailed field investigation and more sophisticated modeling in order to pursue further understanding of the complex behavior of megaflooding in the Channeled Scabland.",Cataclysmic Scabland flooding: Insights from a simple depth-averaged numerical model
"Various monitoring and performance evaluation tools generate considerable amount of low priority traffic. This information is not always needed in real time and often can be delayed by the network without hurting functionality. This paper proposes a new framework to handle this low priority, but resource consuming traffic in such a way that it incurs a minimal interference with the higher priority traffic. Consequently, this improves the network goodput. The key idea is allowing the network nodes to delay data by locally storing it. This can be done, for example, in the Active Network paradigm.In this paper we show that such a model can improve the network's goodput dramatically even if a very simple scheduling algorithm for intermediate parking is used. The parking imposes additional load on the intermediate nodes. To obtain minimal cost schedules we define an optimization problem called the traveling miser problem.We concentrate on the on-line version of the problem for a predefined route, and develop a number of enhanced scheduling strategies. We study their characteristics under different assumptions on the environment through a rigorous simulation study.We prove that if only one link can be congested, then our scheduling algorithm is O(log2 B) competitive, where B is congestion time, and is 3-competitive, if additional signaling is allowed.",The traveling miser problem
"We introduce the first algorithm that we are aware of to employ Bloom filters for longest prefix matching (LPM). The algorithm performs parallel queries on Bloom filters, an efficient data structure for membership queries, in order to determine address prefix membership in sets of prefixes sorted by prefix length. We show that use of this algorithm for Internet Protocol (IP) routing lookups results in a search engine providing better performance and scalability than TCAM-based approaches. The key feature of our technique is that the performance, as determined by the number of dependent memory accesses per lookup, can be held constant for longer address lengths or additional unique address prefix lengths in the forwarding table given that memory resources scale linearly with the number of prefixes in the forwarding table. Our approach is equally attractive for Internet Protocol Version 6 (IPv6) which uses 128-bit destination addresses, four times longer than IPv4. We present a basic version of our approach along with optimizations leveraging previous advances in LPM algorithms. We also report results of performance simulations of our system using snapshots of IPv4 BGP tables and extend the results to IPv6. Using less than 2 Mb of embedded RAM and a commodity SRAM device, our technique achieves average performance of one hash probe per lookup and a worst case of two hash probes and one array access per lookup.",Longest prefix matching using bloom filters
"The medial axis can be viewed as a compact representation for an arbitrary model; it is an essential geometric structure in many applications. A number of practical algorithms for its computation have been aimed at speeding up its computation and at addressing its instabilities. In this paper we propose a new algorithm to compute the medial axis with arbitrary precision. It exhibits several desirable properties not previously combined in a practical and efficient algorithm. First, it allows for a tradeoff between computation time and accuracy, making it well-suited for applications in which an approximation of the medial axis suffices, but computational efficiency is of particular concern. Second, it is output sensitive: the computation complexity of the algorithm does not depend on the size of the representation of a model, but on the size of the representation of the resulting medial axis. Third, the densities of the approximated medial axis points in different areas are adaptive to local free space volumes, based on the assumption that a coarser approximation in wide open area can still suffice the requirements of the applications. We present theoretical results, bounding the error introduced by the approximation process. The algorithm has been implemented and experimental results are presented that illustrate its computational efficiency and robustness.",Efficient and robust computation of an approximated medial axis
"Whilst multimedia technology has been one of the main contributing factors behind the Web's success, delivery of personalized multimedia content has been a desire seldom achieved in practice. Moreover, the perspective adopted is rarely viewed from a cognitive styles standpoint, notwithstanding the fact that they have significant effects on users' preferences with respect to the presentation of multimedia content. Indeed, research has thus far neglected to examine the effect of cognitive styles on users' subjective perceptions of multimedia quality. This paper aims to examine the relationships between users' cognitive styles, the multimedia quality of service delivered by the underlying network, and users' quality of perception (understood as both enjoyment and informational assimilation) associated with the viewed multimedia content. Results from the empirical study reported here show that all users, regardless of cognitive style, have higher levels of understanding of informational content in multimedia video clips (represented in our study by excerpts from television programmes) with weak dynamism, but that they enjoy moderately dynamic clips most. Additionally, multimedia content was found to significantly influence users' levels of understanding and enjoyment. Surprisingly, our study highlighted the fact that Bimodal users prefer to draw on visual sources for informational purposes, and that the presence of text in multimedia clips has a detrimental effect on the knowledge acquisition of all three cognitive style groups.",A cognitive approach to user perception of multimedia quality: An empirical investigation
"This paper examines a discrete material manufacturing system consisting of three machines that are subject to breakdown and one buffer of finite capacity. It is assumed that the buffer has two immediate preceding machines performing the same operations and one immediate succeeding machine receiving material from the buffer. When the buffer reaches its own capacity, one of the two preceding machines has priority over the other to dispose its processed part into the buffer.It is also assumed that there is a new way of the machines reaching failure, by allowing the machines to fail not only when they are operational but also when are either blocked or starved. The latter gives rise to the possibility of modeling the production of more than one part types. The model is solved analytically by developing a recursive algorithm that generates the transition matrix for any value C of the intermediate buffer capacity. Then various performance measures of the system (e.g., throughput) can be easily evaluated. Numerical results for the throughput are also given and these are compared against simulation. The proposed model may be used as a decomposition block to solve large flow lines with merge/ split operations (for example, flow lines with quality inspections and rework loops) and multiple part types.","Markovian analysis of a discrete material manufacturing system with merge operations, operation-dependent and idleness failures"
"We present an adaptive coupled level-set/volume-of-fluid (ACLSVOF) method for interfacial flow simulations on unstructured triangular grids. At each time step, we evolve both the level set function and the volume fraction. The level set function is evolved by solving the level set advection equation using a discontinuous Galerkin finite element method. The volume fraction advection is performed using a Lagrangian-Eulerian method. The interface is reconstructed based on both the level set and the volume fraction information. In particular, the interface normal vector is calculated from the level set function while the line constant is determined by enforcing mass conservation based on the volume fraction. Different from previous works, we have developed an analytic method for finding the line constant on triangular grids, which makes interface reconstruction efficient and conserves volume of fluid exactly. The level set function is finally reinitialized to the signed distance to the reconstructed interface. Since the level set function is continuous, the normal vector calculation is easy and accurate compared to a classic volume-of-fluid method, while tracking the volume fraction is essential for enforcing mass conservation. The method is also coupled to a finite element based Stokes flow solver. The code validation shows that our method is second order and mass is conserved very accurately. In addition, owing to the adaptive grid algorithm we can resolve complex interface changes and interfaces of high curvature efficiently and accurately.",An adaptive coupled level-set/volume-of-fluid interface capturing method for unstructured triangular grids
"Computer Sciences (CS) are having a major impact on recent scientific developments. As any other basic science, the CS is publicly perceived as an open and free science that is trying to make progress in any possible direction, resolving scientific problems in its arena and dealing with pure scientific and issues and theoretical questions per se. In this paper I claim that this is not the case. CS is heavily controlled by commercial forces, which dictate its progress, goals and future directions. The new developments in the CS arena are in most cases the result of the market needs. Several research projects at the universities are funded by the industry, dealing with issues with a clear industrial aspect. Public funds, such as the 6th European Framework project allocates a major portion of their funds for industrial research. The IEEE and ACM, the most respectful technology organizations of the CS, are governed by the industry and the universities at about equal share. The industry is holding patent rights for several CS research results, and CS scientists are issuing patents for their research because it may have a significant economic value. The CS is not an open science, in terms of Popper definition.",Computer sciences and commercial forces: can computer science be considered science?
"In this paper, we present a fast and versatile algorithm which can rapidly perform a variety of nearest neighbor searches. Efficiency improvement is achieved by utilizing the distance lower bound to avoid the calculation of the distance itself if the lower bound is already larger than the global minimum distance. At the preprocessing stage, the proposed algorithm constructs a lower bound tree (LB-tree) by agglomeratively clustering all the sample points to be searched. Given a query point, the lower bound of its distance to each sample point can be calculated by using the internal node of the LB-tree. To reduce the amount of lower bounds actually calculated, the winner-update search strategy is used for traversing the tree. For further efficiency improvement, data transformation can be applied to the sample and the query points. In addition to finding the nearest neighbor, the proposed algorithm can also (i) provide the k-nearest neighbors progressively; (ii) find the nearest neighbors within a specified distance threshold; and (iii) identify neighbors whose distances to the query are sufficiently close to the minimum distance of the nearest neighbor. Our experiments have shown that the proposed algorithm can save substantial computation, particularly when the distance of the query point to its nearest neighbor is relatively small compared with its distance to most other samples (which is the case for many object recognition problems).",Fast and versatile algorithm for nearest neighbor search based on a lower bound tree
"Composition of mappings between schemas is essential to support schema evolution, data exchange, data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this article, we study the issues involved in composing such mappings. Our algorithms and results extend those of Fagin et al. [2004], who studied the composition of mappings given by several kinds of constraints. In particular, they proved that full source-to-target tuple-generating dependencies (tgds) are closed under composition, but embedded source-to-target tgds are not. They introduced a class of second-order constraints, SO tgds, that is closed under composition and has desirable properties for data exchange. We study constraints that need not be source-to-target and we concentrate on obtaining (first-order) embedded dependencies. As part of this study, we also consider full dependencies and second-order constraints that arise from Skolemizing embedded dependencies. For each of the three classes of mappings that we study, we provide: (a) an algorithm that attempts to compute the composition; and (b) sufficient conditions on the input mappings which guarantee that the algorithm will succeed. In addition, we give several negative results. In particular, we show that full and second-order dependencies that are not limited to be source-to-target are not closed under composition (for the latter, under the additional restriction that no new function symbols are introduced). Furthermore, we show that determining whether the composition can be given by these kinds of dependencies is undecidable.",Composition of mappings given by embedded dependencies
"Ventricular late potentials (VLPs) are low-amplitude, high-frequency waveforms appearing in the terminal part of the QRS complex in electrocardiogram (ECG) of patients who are susceptible to ventricular tachycardia and sudden cardiac death, after surviving myocardial infarction. Accordingly, VLP detection presents a prominent non-invasive marker for some cardiac diseases clinically. This paper proposes a VLP detection method based on the wavelet transform and investigates its performance. In this method, a modified vector magnitude waveform is formed using discrete wavelet transform for each high-resolution ECG (HRECG) record; then, by applying the continuous wavelet transform to the QRS complex end part in this waveform, a feature vector is extracted from the resultant time-scale plot. This wavelet-based feature vector is processed by principle component analysis to reduce its dimensionality. Finally, a supervised feedforward artificial neural network, trained by a proper set of these feature vectors, is employed as a classifier. To evaluate the proposed method performance, a HRECG database consisting of the real VLP-negative and simulated VLP-positive patterns is used. In a comparative approach, different VLP detection techniques including the conventional time-domain method, developed by Simson, and some methods utilizing distinct diagnostic features are also applied to this database to investigate the capability of the proposed method in VLP analysis more completely. The results show the proposed method, employing the wavelet transform in both pre-processing and feature extraction stages, reveals high evaluation criteria (accuracy, sensitivity, and specificity) and is qualified to detect VLPs.",Quantitative evaluation of a wavelet-based method in ventricular late potential detection
"The large size and high complexity of security-sensitive applications and systems software is a primary cause for their poor testability and high vulnerability. One approach to alleviate this problem is to extract the security-sensitive parts of application and systems software, thereby reducing the size and complexity of software that needs to be trusted. At the system software level, we use the Nizza architecture which relies on a kernelized trusted computing base (TCB) and on the reuse of legacy code using trusted wrappers to minimize the size of the TCB. At the application level, we extract the security-sensitive portions of an already existing application into an AppCore. The AppCore is executed as a trusted process in the Nizza architecture while the rest of the application executes on a virtualized, untrusted legacy operating system. In three case studies of real-world applications (e-commerce transaction client, VPN gateway and digital signatures in an e-mail client), we achieved a considerable reduction in code size and complexity. In contrast to the few hundred thousand lines of current application software code running on millions of lines of systems software code, we have AppCores with tens of thousands of lines of code running on a hundred thousand lines of systems software code. We also show the performance penalty of AppCores to be modest (a few percent) compared to current software.",Reducing TCB complexity for security-sensitive applications: three case studies
"The layered architecture of middleware platforms (such as CORBA, SOAP, J2EE) is a mixed blessing. On the one hand, layers provide services such as demarshaling, session management, request despatching, quality-of-service (QoS) etc. In a typical middleware platform, every request passes through each layer, whether or not the services provided by that layer are needed for that specific request. This rigid layer processing can lower overall system throughput, and reduce availability and/or increase vulnerability to denial-of-service attacks. For use cases where the response is a simple function of the request input parameters, bypassing middleware layers may be permissible and highly advantageous. Unfortunately, if an application developer desires to selectively bypass the middleware, and process some requests in the lower layer, she has to write platform-specific, intricate low-level code. To evade this trap, we propose to extend the middleware platform with new aspect-oriented modeling syntax, code generation tools, and a development process for building bypassing implementations. Bypassing implementations provide better use of server's resources, leading to better overall client experience. Our core contribution is this idea: aspect-oriented extensions to IDL, additional code generation, along with an enhanced run-time, can enable application developers to conveniently bypass middleware layers when they are not needed, thus improving the server's performance and providing more ""operational headroom"".",An aspect-oriented approach to bypassing middleware layers
"In fuzzy clustering, the fuzzy c-means (FCM) clustering algorithm is the best known and used method. Since the FCM memberships do not always explain the degrees of belonging for the data well, Krishnapuram and Keller proposed a possibilistic approach to clustering to correct this weakness of FCM. However, the performance of Krishnapuram and Keller's approach depends heavily on the parameters. In this paper, we propose another possibilistic clustering algorithm (PCA) which is based on the FCM objective function, the partition coefficient (PC) and partition entropy (PE) validity indexes. The resulting membership becomes the exponential function, so that it is robust to noise and outliers. The parameters in PCA can be easily handled. Also, the PCA objective function can be considered as a potential function, or a mountain function, so that the prototypes of PCA can be correspondent to the peaks of the estimated function. To validate the clustering results obtained through a PCA, we generalized the validity indexes of FCM. This generalization makes each validity index workable in both fuzzy and possibilistic clustering models. By combining these generalized validity indexes, an unsupervised possibilistic clustering is proposed. Some numerical examples and real data implementation on the basis of the proposed PCA and generalized validity indexes show their effectiveness and accuracy.",Unsupervised possibilistic clustering
"In the first part of this paper we determine the largest step size of Runge-Kutta (RK) methods for which the corresponding numerical approximations are positive (component-wise non-negative) for arbitrary positive initial vector, whenever the underlying initial value problem (IVP) possesses the related positivity preserving property. We prove that step size thresholds for certain classes of positive IVPs guaranteeing positivity that we derived in a former paper are strict for irreducible and non-confluent RK methods. Investigating the strict positivity step size thresholds we can see that these are rather small if at all positive: often they are, roughly speaking, inverse proportional to the Lipschitz constant of the problem.However, for certain (stiff) IVPs with some particular initial vectors, e.g., for some ""smooth"" vectors in semi-discretized diffusion problems, we experience preservation of positivity with much larger step sizes than the strict positivity step size threshold. To catch this phenomenon, in the second part of the paper we construct positively invariant sets of positive vectors and derive step size thresholds for the discrete version of the positive invariance. The resulting threshold for discrete positive invariance is, roughly speaking, inverse proportional to the one-sided Lipschitz constant only and is shown in good accordance with some displayed computational experiments.",On the positivity step size threshold of Runge-Kutta methods
"Image classification arises as an important phase in the overall process of automatic image annotation and image retrieval. In this study, we are concerned with the design of image classifiers developed in the feature space formed by low level primitives defined in the setting of the MPEG-7 standard. Our objective is to investigate the discriminatory properties of such standard image descriptors and look at efficient architectures of the classifiers along with their design pursuits. The generalization capabilities of an image classifier are essential to its successful usage in image retrieval and annotation. Intuitively, it is expected that the classifier should achieve high classification accuracy on unseen images that are quite ''similar'' to those occurring in the training set. On the other hand, we may assume that the performance of the classifier could not be guaranteed in the case of images that are very much dissimilar from the elements of the training set. To follow this observation, we develop and use a concept of the localized generalization error and show how it guides the design of the classifier. As image classifier, we consider the usage of the radial basis function neural networks (RBFNNs). Through intensive experimentation we show that the resulting classifier outperforms other classifiers such as a multi-class support vector machines (SVMs) as well as ''standard'' RBFNNs (viz. those developed without the guidance offered by the optimization of the localized generalization error). The experimental studies reveal some interesting interpretation abilities of the RBFNN classifiers being related with their receptive fields.",Image classification with the use of radial basis function neural networks and the minimization of the localized generalization error
"Although independent consideration of layers simplifies wireless system design, it is inadequate since: 1) it does not consider the effect of co-channel user interference on higher layers; 2) it does not address the impact of local adaptation actions on overall performance; and 3) it attempts to optimize performance at one layer while keeping parameters of other layers fixed. Cross-layer adaptation techniques spanning several layers improve performance and provide better quality of service for users across layers. In this study, we consider a synergy between the physical and access layers and address the joint problem of channel allocation, modulation level, and power control in a multicell network. Since performance is determined by channel reuse, it is important to handle co-channel interference appropriately by constructing co-channel user sets and by assigning transmission parameters so that achievable system rate is maximized. The problem is considered for orthogonal frequency-division multiplexing, which introduces novel challenges to resource allocation due to different quality of subcarriers for users and existing transmit power constraints. We study the structure of the problem and present two classes of centralized heuristic algorithms. The first one considers each subcarrier separately and sequentially allocates users from different base stations in the subcarrier based on different criteria, while the second is based on water-filling across subcarriers in each cell. Our results show that the first class of heuristics performs better and quantify the impact of different parameters on system performance.",Cross-layer adaptive techniques for throughput enhancement in wireless OFDM-based networks
"We describe LEAP&plus; (Localized Encryption and Authentication Protocol), a key management protocol for sensor networks that is designed to support in-network processing, while at the same time restricting the security impact of a node compromise to the immediate network neighborhood of the compromised node. The design of the protocol is motivated by the observation that different types of messages exchanged between sensor nodes have different security requirements, and that a single keying mechanism is not suitable for meeting these different security requirements. LEAP&plus; supports the establishment of four types of keys for each sensor node: an individual key shared with the base station, a pairwise key shared with another sensor node, a cluster key shared with multiple neighboring nodes, and a global key shared by all the nodes in the network. LEAP&plus; also supports (weak) local source authentication without precluding in-network processing. Our performance analysis shows that LEAP&plus; is very efficient in terms of computational, communication, and storage costs. We analyze the security of LEAP&plus; under various attack models and show that LEAP&plus; is very effective in defending against many sophisticated attacks, such as HELLO flood attacks, node cloning attacks, and wormhole attacks. A prototype implementation of LEAP&plus; on a sensor network testbed is also described.",LEAP+: Efficient security mechanisms for large-scale distributed sensor networks
"We study the economic interests of a wireless access point owner and his paying client, and model their interaction as a dynamic game. The key feature of this game is that the players have asymmetric information-the client knows more than the access provider. We find that if a client has a ""web browser"" utility function (a temporal utility function that grows linearly), it is a Nash equilibrium for the provider to charge the client a constant price per unit time. On the other hand, if the client has a ""file transferor"" utility function (a utility function that is a step function), the client would be unwilling to pay until the final time slot of the file transfer. We also study an expanded game where an access point sells to a reseller, which in turn sells to a mobile client and show that if the client has a web browser utility function, that constant price is a Nash equilibrium of the three player game. Finally, we study a two player game in which the access point does not know whether he faces a web browser or file transferor type client, and show conditions for which it is not a Nash equilibrium for the access point to maintain a constant price.",WiFi access point pricing as a dynamic game
"In this paper, we present a framework for providing fair service and supporting quality of service (QoS) requirements in IEEE 802.11 networks with multiple access points (APs). These issues becomes critical as IEEE 802.11 wireless LAN are widely deployed in nationwide networks, linking tens of thousands of ""hotspots"" for providing both real-time (voice) and non real-time (data) services to a large population of mobile users. However, both fairness and QoS guarantees cannot be supported in the current 802.11 standard.Our system, termed MiFi, relies on centralized coordination of the APs. During any given time of the ""contention-free"" period only a set of non-interfering APs is activated while the others are silenced. Moreover, the amount of service granted to an AP is proportional to its load and the system's performance is optimized by employing efficient scheduling algorithms. We show that such a system can be implemented without requiring any modification of the underlying MAC protocol standard or the behavior of the mobile stations. Our scheme is complementary to the emerging 802.11e standard for QoS and guarantees to overcome the hidden node and the overlapping cell problems. Our simulations establish that the system supports fairness and hence can provide QoS guarantees for real-time traffic, while maintaining a relative high throughput.",MiFi: a framework for fairness and QoS assurance for current IEEE 802.11 networks with multiple access points
"The performance of many supervised and unsupervised learning algorithms is very sensitive to the choice of an appropriate distance metric. Previous work in metric learning and adaptation has mostly been focused on classification tasks by making use of class label information. In standard clustering tasks, however, class label information is not available. In order to adapt the metric to improve the clustering results, some background knowledge or side information is needed. One useful type of side information is in the form of pairwise similarity or dissimilarity information. Recently, some novel methods (e.g., the parametric method proposed by Xing et al.) for learning global metrics based on pairwise side information have been shown to demonstrate promising results. In this paper, we propose a nonparametric method, called relaxational metric adaptation (RMA), for the same metric adaptation problem. While RMA is local in the sense that it allows locally adaptive metrics, it is also global because even patterns not in the vicinity can have long-range effects on the metric adaptation process. Experimental results for semi-supervised clustering based on both simulated and real-world data sets show that RMA outperforms Xing et al.'s method under most situations. Besides applying RMA to semi-supervised learning, we have also used it to improve the performance of content-based image retrieval systems through metric adaptation. Experimental results based on two real-world image databases show that RMA significantly outperforms other methods in improving the image retrieval performance.",Relaxational metric adaptation and its application to semi-supervised clustering and content-based image retrieval
"A central controller chooses a state-dependent transmission rate for each user in a fading, downlink channel by varying transmission power over time. For each user, the state of the channel evolves over time according to an exogenous continuous-time Markov chain (CTMC), which affects the quality of transmission. The traffic for each user, arriving at the central controller, is modeled as a finite-buffer Markovian queue with adjustable service rates. That is, for each user data packets arrive to the central controller according to a Poisson process and packet size is exponentially distributed; an arriving packet is dropped if the associated buffer is full, which results in degradation of quality of service. The controller forwards (downlink) the arriving packets to the corresponding user according to an optimally chosen transmission rate from a fixed set A i of available values for each user i, depending on the backlog in the system and the channel state of all users. The objective is to maximize quality of service subject to an upper bound on the long-run average power consumption. We show that the optimal transmission rate for each user is solely a function of his own packet queue length and channel state; the dependence among users is captured through a penalty rate. Further, we explicitly characterize the optimal transmission rate for each user.",Dynamic power control in a fading downlink channel subject to an energy constraint
"Geolocation of Internet hosts enables a new class of location-aware applications. Previous measurement-based approaches use reference hosts, called landmarks, with a well-known geographic location to provide the location estimation of a target host. This leads to a discrete space of answers, limiting the number of possible location estimates to the number of adopted landmarks. In contrast, we propose Constraint-Based Geolocation (CBG), which infers the geographic location of Internet hosts using multilateration with distance constraints to establish a continuous space of answers instead of a discrete one. However, to use multilateration in the Internet, the geographic distances from the landmarks to the target host have to be estimated based on delay measurements between these hosts. This is a challenging problem because the relationship between network delay and geographic distance in the Internet is perturbed by many factors, including queueing delays and the absence of great-circle paths between hosts. CBG accurately transforms delay measurements to geographic distance constraints, and then uses multilateration to infer the geolocation of the target host. Our experimental results show that CBG outperforms previous geolocation techniques. Moreover, in contrast to previous approaches, our method is able to assign a confidence region to each given location estimate. This allows a location-aware application to assess whether the location estimate is sufficiently accurate for its needs.",Constraint-based geolocation of internet hosts
"We propose to extend the well-known MUSCL-Hancock scheme for Euler equations to the induction equation modeling the magnetic field evolution in kinematic dynamo problems. The scheme is based on an integral form of the underlying conservation law which, in our formulation, results in a ""finite-surface"" scheme for the induction equation. This naturally leads to the well-known ""constrained transport"" method, with additional continuity requirement on the magnetic field representation. The second ingredient in the MUSCL scheme is the predictor step that ensures second order accuracy both in space and time. We explore specific constraints that the mathematical properties of the induction equations place on this predictor step, showing that three possible variants can be considered. We show that the most aggressive formulations (referred to as C-MUSCL and U-MUSCL) reach the same level of accuracy as the other one (referred to as Runge-Kutta), at a lower computational cost. More interestingly, these two schemes are compatible with the adaptive mesh refinement (AMR) framework. It has been implemented in the AMR code RAMSES. It offers a novel and efficient implementation of a second order scheme for the induction equation. We have tested it by solving two kinematic dynamo problems in the low diffusion limit. The construction of this scheme for the induction equation constitutes a step towards solving the full MHD set of equations using an extension of our current methodology.",Kinematic dynamos using constrained transport with high order Godunov schemes and adaptive mesh refinement
"This article presents a long&dash;term inter&dash;disciplinary research project situated at the intersection of the scientific disciplines of Musicology and Artificial Intelligence. The goal is to develop AI, and in particular machine learning and data mining, methods to study the complex phenomenon of expressive music performance. Formulating formal, quantitative models of expressive performance is one of the big open research problems in contemporary (empirical and cognitive) musicology. Our project develops a new direction in this field: we use inductive learning techniques to discover general and valid expression principles from (large amounts of) real performance data. The project is currently starting its third year and is planned to continue for at least four more years.In the following, we explain the basic notions of expressive music performance, and why this is such a central phenomenon in music. We present the general research framework of the project, and discuss the various challenges and research opportunities that emerge in this framework. We then briefly describe the current state of the project and list the main achievements made so far. In the rest of the paper, we discuss in more detail one particular data mining approach (including a new algorithm for learning characterisation rules) that we have developed recently. Preliminary experimental results demonstrate that this algorithm can discover very general and robust expression principles, some of which actually constitute novel discoveries from a musicological viewpoint.",Using AI and machine learning to study expressive music performance: project survey and first report
"Healthcare organizations are often organized in a modular, loosely coupled fashion where separate and semi-autonomous work units specialize in different areas of care delivery. This partitioning allows each unit to adapt to emerging practice standards in its area of expertise and to adjust to its local work environment. However, organizational loose coupling can limit the flow of information within organizations and can make it difficult to coordinate services when patients' care is dependent on professionals from more than one unit. Groupware systems have the potential to improve coordination and information access in healthcare organizations. However, modularity and loose coupling make it difficult to introduce new systems when they span more than one unit, since authority is not always centralized and since perceptions and frames of reference on new deployments differ across units. In this paper, we define a groupware deployment framework for loosely coupled healthcare organizations that has two parts: a set of deployment challenges and a set of deployment strategies. The deployment challenges include: difficulties centralizing deployments, perceptions of inequity, role conflicts, and problems achieving critical mass. The deployment strategies outline a preliminary set of approaches for addressing the difficulties of deploying CSCW systems in loosely coupled healthcare organizations. We illustrate the framework by presenting a case study of a groupware deployment in a home care setting.",Loose Coupling and Healthcare Organizations: Deployment Strategies for Groupware
"Genetic algorithms are commonly used metaheuristics for global optimization, but there has been very little research done on the generation of their initial population. In this paper, we look for an answer to the question whether the initial population plays a role in the performance of genetic algorithms and if so, how it should be generated. We show with a simple example that initial populations may have an effect on the best objective function value found for several generations. Traditionally, initial populations are generated using pseudo random numbers, but there are many alternative ways. We study the properties of different point generators using four main criteria: the uniform coverage and the genetic diversity of the points as well as the speed and the usability of the generator. We use the point generators to generate initial populations for a genetic algorithm and study what effects the uniform coverage and the genetic diversity have on the convergence and on the final objective function values. For our tests, we have selected one pseudo and one quasi random sequence generator and two spatial point processes: simple sequential inhibition process and nonaligned systematic sampling. In numerical experiments, we solve a set of 52 continuous test functions from 16 different function families, and analyze and discuss the results.",On initial populations of a genetic algorithm for continuous optimization problems
"a lattice Boltzmann model for simulating multiphase flows with large density ratios is described in this paper. The method is easily implemented. It does not require solving the Poisson equation and does not involve the complex treatments of derivative terms. The interface capturing equation is recovered without any additional terms as compared to other methods [M.R. Swift, W.R. Osborn, J.M. Yeomans, Lattice Boltzmann simulation of liquid-gas and binary fluid systems, Phys. Rev. E 54 (1996) 5041-5052; T. Inamuro, T. Ogata, S. Tajima, N. Konishi, A lattice Boltzmann method for incompressible two-phase flows with large density differences, J. Comput. Phys. 198 (2004) 628-644; T. Lee, C.-L. Lin, A stable discretization of the lattice Boltzmann equation for simulation of incompressible two-phase flows at high density ratio, J. Comput. Phys. 206 (2005) 16-47]. Besides, it requires less discrete velocities. As a result, its efficiency could be greatly improved, especially in 3D applications. It is validated by several cases: a bubble in a stationary flow and the capillary wave. The numerical surface tension obtained from the Laplace law and the interface profile agrees very well with the respective analytical solution. The method is further verified by its application to capillary wave and the bubble rising under buoyancy with comparison to other methods. All the numerical experiments show that the present approach can be used to model multiphase flows with large density ratios.",A lattice Boltzmann model for multiphase flows with large density ratio
"This paper presents a novel face detection method by applying discriminating feature analysis (DFA) and support vector machine (SVM). The novelty of our DFA-SVM method comes from the integration of DFA, face class modeling, and SVM for face detection. First, DFA derives a discriminating feature vector by combining the input image, its 1-D Haar wavelet representation, and its amplitude projections. While the Haar wavelets produce an effective representation for object detection, the amplitude projections capture the vertical symmetric distributions and the horizontal characteristics of human face images. Second, face class modeling estimates the probability density function of the face class and defines a distribution-based measure for face and nonface classification. The distribution-based measure thus separates the input patterns into three classes: the face class (patterns close to the face class), the nonface class (patterns far away from the face class), and the undecided class (patterns neither close to nor far away from the face class). Finally, SVM together with the distribution-based measure classifies the patterns in the undecided class into either the face class or the nonface class. Experiments using images from the MIT-CMU test sets demonstrate the feasibility of our new face detection method. In particular, when using 92 images (containing 282 faces) from the MIT-CMU test sets, our DFA-SVM method achieves 98.2% correct face detection rate with two false detections.",Face detection using discriminating feature analysis and Support Vector Machine
"As modern operating systems and software become larger and more complex, they are more likely to contain bugs, which may allow attackers to gain illegitimate access. A fast and reliable mechanism to discern and generate vaccines for such attacks is vital for the successful protection of networks and systems. In this paper we present Argos, a containment environment for worms as well as human orchestrated attacks. Argos is built upon a fast x86 emulator which tracks network data throughout execution to identify their invalid use as jump targets, function addresses, instructions, etc. Furthermore, system call policies disallow the use of network data as arguments to certain calls. When an attack is detected, we perform 'intelligent' process- or kernel-aware logging of the corresponding emulator state for further offline processing. In addition, our own forensics shellcode is injected, replacing the malevolent shellcode, to gather information about the attacked process. By correlating the data logged by the emulator with the data collected from the network, we are able to generate accurate network intrusion detection signatures for the exploits that are immune to payload mutations. The entire process can be automated and has few if any false positives, thus rapid global scale deployment of the signatures is possible.",Argos: an emulator for fingerprinting zero-day attacks for advertised honeypots with automatic signature generation
"In this paper, we study problems related to supporting unicast and multicast connections with quality of service (QoS) requirements. We investigate the problem of optimal routing and resource allocation in the context of performance dependent costs. In this context, each network element can offer several QoS guarantees, each associated with a different cost. This is a natural extension to the commonly used bi-criteria model, where each link is associated with a single delay and a single cost. This framework is simple yet strong enough to model many practical interesting networking problems.An important problems in this framework is finding a good path for a connection that minimizes the cost while retaining the end-to-end delay requirement. Once such a path (or a tree, in the multicast case) is found, one needs to partition the end-to-end QoS requirements among the links of the path (tree). We consider the case of general integer cost functions (where delays and cost are integers). As the related problem is NP complete, we concentrate on finding efficient Îµ-approximation solutions. We improve on recent previous results by ErgÃ¼n et al. Lorenz and Orda, and Raz and Shavitt, both in terms of generality as well as in terms of complexity of the solution. In particular, we present novel approximation techniques that yield the best known complexity for the unicast QoS routing problem, and the first approximation algorithm for the QoS partition problem on trees, both for the centralized and distributed cases.",Efficient QoS partition and routing of unicast and multicast
"Service prioritization among different traffic classes is an important goal for the Internet. Conventional approaches to solving this problem consider the existing best-effort class as the low-priority class, and attempt to develop mechanisms that provide ""better-than-best-effort"" service. In this paper, we explore the opposite approach, and devise a new distributed algorithm to realize a low-priority service (as compared to the existing best effort) from the network endpoints. To this end, we develop TCP Low Priority (TCP-LP), a distributed algorithm whose goal is to utilize only the excess network bandwidth as compared to the ""fair share"" of bandwidth as targeted by TCP. The key mechanisms unique to TCP-LP congestion control are the use of one-way packet delays for early congestion indications and a TCP-transparent congestion avoidance policy. The results of our simulation and Internet experiments show that: 1) TCP-LP is largely non-intrusive to TCP traffic; 2) both single and aggregate TCP-LP flows are able to successfully utilize excess network bandwidth; moreover, multiple TCP-LP flows share excess bandwidth fairly; 3) substantial amounts of excess bandwidth are available to the low-priority class, even in the presence of ""greedy"" TCP flows; 4) the response times of web connections in the best-effort class decrease by up to 90% when long-lived bulk data transfers use TCP-LP rather than TCP; 5) despite their low-priority nature, TCP-LP flows are able to utilize significant amounts of available bandwidth in a wide-area network environment.",TCP-LP: low-priority service via end-point congestion control
"A conventional way to discriminate between objects represented by dissimilarities is the nearest neighbor method. A more efficient and sometimes a more accurate solution is offered by other dissimilarity-based classifiers. They construct a decision rule based on the entire training set, but they need just a small set of prototypes, the so-called representation set, as a reference for classifying new objects. Such alternative approaches may be especially advantageous for non-Euclidean or even non-metric dissimilarities. The choice of a proper representation set for dissimilarity-based classifiers is not yet fully investigated. It appears that a random selection may work well. In this paper, a number of experiments has been conducted on various metric and non-metric dissimilarity representations and prototype selection methods. Several procedures, like traditional feature selection methods (here effectively searching for prototypes), mode seeking and linear programming are compared to the random selection. In general, we find out that systematic approaches lead to better results than the random selection, especially for a small number of prototypes. Although there is no single winner as it depends on data characteristics, the k-centres works well, in general. For two-class problems, an important observation is that our dissimilarity-based discrimination functions relying on significantly reduced prototype sets (3-10% of the training objects) offer a similar or much better classification accuracy than the best k-NN rule on the entire training set. This may be reached for multi-class data as well, however such problems are more difficult.",Prototype selection for dissimilarity-based classifiers
"Embedded systems are rapidly growing in size, complexity, distribution, and heterogeneity. As a result, the traditional practice of developing one-off embedded applications that are often rigid and unmanageable is no longer acceptable. Recent studies have suggested that an effective approach to developing software systems in this domain is to employ the principles of software architecture. However, for software architectural concepts to be truly useful in a development setting, they must be accompanied by support for their implementation and evolution. This has motivated our work over the past several years on an architectural middleware, called Prism-MW, that provides implementation-level support for the development of software systems in terms of the software architectural constructs (e.g., components, connectors). Prism-MW was initially developed in Java and used in several domains. Recently, as part of an on-going project, we were required to implement Prism-MW in ANSI C++. This experience proved to be more challenging than we initially anticipated, mainly due to the inherent heterogeneity of the computing substrate. As a result of this experience, we had to reconsider some of our earlier assumptions of what constitutes an architectural middleware and its role in the software development process. In this paper, we provide an overview of our experience and the lessons we have learned along the way.",Tailoring an architectural middleware platform to a heterogeneous embedded environment
"In this paper we present Parallel Computing with Mobile Agents (PaCMAn), a mobile agent based Metacomputer that enables its users to utilize idle resources on the internet to tackle computational problems that could not be handled efficiently with their own resources. The PaCMAn launches multiple mobile agents that cooperate and communicate to solve problems in parallel. Each agent supports the basic communication and synchronization tasks of the classical parallel worker assuming the role of a process in a parallel processing application. Application tasks, however, are assigned dynamically to the PaCMAn's mobile agents via TaskHandlers. TaskHandlers are Java objects capable of implementing particular tasks of the application. The PaCMAn consists of three major components: Broker, Server and Client. A server machine has to be explicitly registered in order to take part in the PaCMAn Metacomputer. A number of brokers keep track of the available resources. In the PaCMAn system both server and client machines can be located anywhere in the Internet. The clients select the servers that they will utilize based on the specific resource requirements. We have developed and tested prototype systems with several applications. These prototypes provide proof of concept of our proposed Metacomputing philosophy. Furthermore they have demonstrated that PaCMAn provides parallel efficiency. We also demonstrate that the PaCMAn Metacomputer can be used as the computational engine for the creation of sophisticated Pervasive Services anywhere anytime.",Metacomputing with mobile agents
"We study an acceleration method for point-to-point shortest-path computations in large and sparse directed graphs with given nonnegative arc weights. The acceleration method is called the arc-flag approach and is based on Dijkstra's algorithm. In the arc-flag approach, we allow a preprocessing of the network data to generate additional information, which is then used to speedup shortest-path queries. In the preprocessing phase, the graph is divided into regions and information is gathered on whether an arc is on a shortest path into a given region. The arc-flag method combined with an appropriate partitioning and a bidirected search achieves an average speedup factor of more than 500 compared to the standard algorithm of Dijkstra on large networks (1 million nodes, 2.5 million arcs). This combination narrows down the search space of Dijkstra's algorithm to almost the size of the corresponding shortest path for long-distance shortest-path queries. We conduct an experimental study that evaluates which partitionings are best suited for the arc-flag method. In particular, we examine partitioning algorithms from computational geometry and a multiway arc separator partitioning. The evaluation was done on German road networks. The impact of different partitions on the speedup of the shortest path algorithm are compared. Furthermore, we present an extension of the speedup technique to multiple levels of partitions. With this multilevel variant, the same speedup factors can be achieved with smaller space requirements. It can, therefore, be seen as a compression of the precomputed data that preserves the correctness of the computed shortest paths.",Partitioning graphs to speedup Dijkstra's algorithm
"Microfluidics-based biochips consist of microfluidic arrays on rigid substrates through which, movement of fluids is tightly controlled to facilitate biological reactions. Biochips are soon expected to revolutionize biosensing, clinical diagnostics, and drug discovery. Critical to the deployment of biochips in such diverse areas is the dependability of these systems. Thus, robust testing techniques are required to ensure an adequate level of system dependability. Due to the underlying mixed technology and energy domains, such biochips exhibit unique failure mechanisms and defects. In this article we present a highly effective fault diagnosis strategy that uses a single source and sink to detect and locate multiple faults in a microfluidic array, without flooding the array, a problem that has hampered realistic implementations of all existing strategies. The strategy renders itself well for a built-in self-test that could drastically reduce the operating cost of microfluidic biochips. It can be used during both the manufacturing phase of the biochip, as well as field operation. Furthermore, the algorithm can pinpoint the actual fault, as opposed to merely the faulty regions that are typically identified by strategies proposed in the literature. Also, analytical results suggest that it is an effective strategy that can be used to design highly dependable biochip systems.",Multiple fault diagnosis in digital microfluidic biochips
"This paper presents a novel approach to the fast computation of Zernike moments from a digital image. Most existing fast methods for computing Zernike moments have focused on the reduction of the computational complexity of the Zernike 1-D radial polynomials by introducing their recurrence relations. Instead, in our proposed method, we focus on the reduction of the complexity of the computation of the 2-D Zernike basis functions. As Zernike basis functions have specific symmetry or anti-symmetry about the x-axis, the y-axis, the origin, and the straight line y=x, we can generate the Zernike basis functions by only computing one of their octants. As a result, the proposed method makes the computation time eight times faster than existing methods. The proposed method is applicable to the computation of an individual Zernike moment as well as a set of Zernike moments. In addition, when computing a series of Zernike moments, the proposed method can be used with one of the existing fast methods for computing Zernike radial polynomials. This paper also presents an accurate form of Zernike moments for a discrete image function. In the experiments, results show the accuracy of the form for computing discrete Zernike moments and confirm that the proposed method for the fast computation of Zernike moments is much more efficient than existing fast methods in most cases.",A novel approach to the fast computation of Zernike moments
"Most existing reliable multicast congestion control (RMCC) mechanisms try to emulate TCP congestion control behaviors for achieving TCP-compatibility. However, different loss recovery mechanisms employed in reliable multicast protocols, especially NAK-based retransmission and local loss recovery mechanisms, may lead to different behaviors and performance of congestion control. As a result, reliable multicast flows might be identified and treated as non-TCP-friendly by routers in the network. It is essential to understand those influences and take them into account in the development and deployment of reliable multicast services. In this paper, we study the influences comprehensively through analysis, modelling and simulations. We demonstrate that NAK-based retransmission and/or local loss recovery mechanisms are much more robust and efficient in recovering from single or multiple packet losses within a single round-trip time (RTT). For a better understanding on the impact of loss recovery on RMCC, we derive expressions for steady-state throughput of NAK-based RMCC schemes, which clearly brings out the throughput advantages of NAK-based RMCC over TCP Reno. We also show that timeout effects have little impact on shaping the performance of NAK-based RMCC schemes except for extremely high loss rates (>0.2). Finally, we use simulations to validate our findings and show that local loss recovery may further increase the throughput and deteriorate the fairness properties of NAK-based RMCC schemes. These findings and insights could provide useful recommendations for the design, testing and deployment of reliable multicast protocols and services.",The impact of loss recovery on congestion control for reliable multicast
"Esterel is a synchronous design language for the specification of reactive systems. There exist two main semantics for Esterel. On the one hand, the logical behavioral semantics provides a simple and compact formalization of the behavior of programs using SOS rules. But it does not ensure deterministic deadlock-free executions, as it may define zero, one, or many possible behaviors for a given program and input sequence. Since nondeterministic programs have to be rejected by compilers, this means that it defines behaviors for incorrect programs, which is awkward. On the other hand, the constructive semantics is deterministic (amongst other properties) but at the expense of a much more complex formalism. In this work, we build and thoroughly analyze a new deterministic semantics for Esterel that retains the simplicity of the logical behavioral semantics from which it derives. It defines, at most, one behavior per program and input sequence. We further extend this semantics with the ability to deal with errors so that incorrect programs are no longer (negatively) characterized by a lack of behavior, but (positively) by the existence of an incorrect behavior. In our view, this new semantics, with or without explicit errors, provides a better framework for formal and automated reasoning about Esterel programs.",A deterministic logical semantics for pure Esterel
"In this paper, we formally present a novel estimation method, referred to as the Stochastic Learning Weak Estimator (SLWE), which yields the estimate of the parameters of a binomial distribution, where the convergence of the estimate is weak, i.e. with regard to the first and second moments. The estimation is based on the principles of stochastic learning. The mean of the final estimate is independent of the scheme's learning coefficient, @l, and both the variance of the final distribution and the speed decrease with @l. Similar results are true for the multinomial case, except that the equations transform from being of a scalar type to be of a vector type. Amazingly enough, the speed of the latter only depends on the same parameter, @l, which turns out to be the only non-unity eigenvalue of the underlying stochastic matrix that determines the time-dependence of the estimates. An empirical analysis on synthetic data shows the advantages of the scheme for non-stationary distributions. The paper also briefly reports (without detailed explanation) conclusive results that demonstrate the superiority of SLWE in pattern-recognition-based data compression, where the underlying data distribution is non-stationary. Finally, and more importantly, the paper includes the results of two pattern recognition exercises, the first of which involves artificial data, and the second which involves the recognition of the types of data that are present in news reports of the Canadian Broadcasting Corporation (CBC). The superiority of the SLWE in both these cases is demonstrated.",Stochastic learning-based weak estimation of multinomial random variables and its applications to pattern recognition in non-stationary environments
"We study the complexity of approximating the smallest eigenvalue of a univariate Sturm-Liouville problem on a quantum computer. This general problem includes the special case of solving a one-dimensional SchrÃ¶dinger equation with a given potential for the ground state energy.The Sturm-Liouville problem depends on a function q, which, in the case of the SchrÃ¶dinger equation, can be identified with the potential function V. Recently Papageorgiou and Wozniakowski proved that quantum computers achieve an exponential reduction in the number of queries over the number needed in the classical worst-case and randomized settings for smooth functions q. Their method uses the (discretized) unitary propagator and arbitrary powers of it as a query (""power queries""). They showed that the Sturm-Liouville equation can be solved with O(log(1/Îµ)) power queries, while the number of queries in the worst-case and randomized settings on a classical computer is polynomial in 1/Îµ. This proves that a quantum computer with power queries achieves an exponential reduction in the number of queries compared to a classical computer.In this paper we show that the number of queries in Papageorgiou's and Wozniakowski's algorithm is asymptotically optimal. In particular we prove a matching lower bound of Î©(log(1/Îµ)) power queries, therefore showing that Î˜(log(1/Îµ)) power queries are sufficient and necessary. Our proof is based on a frequency analysis technique, which examines the probability distribution of the final state of a quantum algorithm and the dependence of its Fourier transform on the input.",A lower bound for the Sturm-Liouville eigenvalue problem on a quantum computer
"Managing power concerns in microprocessors has become a pressing research problem across the domains of computer architecture, CAD, and compilers. As a result, several parameterized cycle-level power simulators have been introduced. While these simulators can be quite useful for microarchitectural studies, their generality limits how accurate they can be for any one chip family. Furthermore, their hardware focus means that they do not explicitly enable studying the interaction of different software layers, such as Java applications and their underlying runtime system software. This paper describes and evaluates XTREM, a power-simulation tool tailored for the Intel XScale microarchitecture. In building XTREM, our goals were to develop a microarchitecture simulator that, while still offering size parameterizations for cache and other structures, more accurately reflected a realistic processor pipeline. We present a detailed set of validations based on multimeter power measurements and hardware performance counter sampling. XTREM exhibits an average performance error of only 6.5&percnt; and an even smaller average power error: 4&percnt;. The paper goes on to present an application study enabled by the simulator. Namely, we use XTREM to produce an energy consumption breakdown for Java CDC and CLDC applications. Our simulator measurements indicate that a large percentage of the total energy consumption (up to 35&percnt;) is devoted to the virtual machine's support functions.",The XTREM power and performance simulator for the Intel XScale core: Design and experiences
"This paper discusses the interest of the Tree of Shapes of an image as a region oriented image representation. The Tree of Shapes offers a compact and structured representation of the family of level lines of an image. This representation has been used for many processing tasks such as filtering, registration, or shape analysis. In this paper we show how this representation can be used for segmentation, rate distortion optimization, and encoding. We address the problem of segmentation and rate distortion optimization using Guigues algorithm on a hierarchy of partitions constructed using the simplified Mumford-Shah multiscale energy. To segment an image, we minimize the simplified Mumford-Shah energy functional on the set of partitions represented in this hierarchy. The rate distortion problem is also solved in this hierarchy of partitions. In the case of encoding, we propose a variational model to select a family of level lines of a gray level image in order to obtain a minimal description of it. Our energy functional represents the cost in bits of encoding the selected level lines while controlling the maximum error of the reconstructed image. In this case, a greedy algorithm is used to minimize the corresponding functional. Some experiments are displayed.",Level Lines Selection with Variational Models for Segmentation and Encoding
"In this paper we propose a new approach in genetic algorithm called distributed hierarchical genetic algorithm (DHGA) for optimization and pattern matching. It is eventually a hybrid technique combining the advantages of both distributed and hierarchical processes in exploring the search space. The search is initially distributed over the space and then in each subspace the algorithm works in a hierarchical way. The entire space is essentially partitioned into a number of subspaces depending on the dimensionality of the space. This is done in order to spread the search process more evenly over the whole space. In each subspace the genetic algorithm is employed for searching and the search process advances from one hypercube to a neighboring hypercube hierarchically depending on the convergence status of the population and the solution obtained so far. The dimension of the hypercube and the resolution of the search space are altered with iterations. Thus the search process passes through variable resolution (coarse-to-fine) search space. Both analytical and empirical studies have been carried out to evaluate the performance between DHGA and distributed conventional GA (DCGA) for different function optimization problems. Further, the performance of the algorithms is demonstrated on problems like pattern matching and object matching with edge map.",A distributed hierarchical genetic algorithm for efficient optimization and pattern matching
"It is a well known classical result that given the image projections of three known world points it is possible to solve for the pose of a calibrated perspective camera to up to four pairs of solutions. We solve the Generalised problem where the camera is allowed to sample rays in some arbitrary but known fashion and is not assumed to perform a central perspective projection. That is, given three back-projected rays that emanate from a camera or multi-camera rig in an arbitrary but known fashion, we seek the possible poses of the camera such that the three rays meet three known world points. We show that the Generalised problem has up to eight solutions that can be found as the intersections between a circle and a ruled quartic surface. A minimal and efficient constructive numerical algorithm is given to find the solutions. The algorithm derives an octic polynomial whose roots correspond to the solutions. In the classical case, when the three rays are concurrent, the ruled quartic surface and the circle possess a reflection symmetry such that their intersections come in symmetric pairs. This manifests itself in that the odd order terms of the octic polynomial vanish. As a result, the up to four pairs of solutions can be found in closed form. The proposed algorithm can be used to solve for the pose of any type of calibrated camera or camera rig. The intended use for the algorithm is in a hypothesise-and-test architecture.",A Minimal Solution to the Generalised 3-Point Pose Problem
"In one hand the graph isomorphism problem (GI) has received considerable attention due to its unresolved complexity status and is many practical applications. On the other hand a notion of compatible topologies on graphs has emerged from digital topology (see [A. Bretto, Comparability graphs and digital topology, Comput. Vision Graphic Image Process. (Image Understanding), 82 (2001) 33-41; J.M. Chassery, Connectivity and consecutivity in digital pictures, Comput. Vision Graphic Image Process. 9 (1979) 294-300; L.J. Latecki, Topological Connectedness and 8-connectness in digital pictures, CVGIP Image Understanding 57(2) (1993) 261-262; U. Eckhardt, L.J. Latecki, Topologies for digital spaces Z2 and Z3, Comput. Vision Image Understanding 95 (2003) 261-262; T.Y. Kong, R. Kopperman, P.R. Meyer, A topological approach to digital topology, Amer. Math. Monthly Archive 98(12) (1991) 901-917; R. Kopperman, Topological digital topology, Discrete geometry for computer imagery, 11th International Conference, Lecture Notes in Computer Science, Vol. 2886, DGCI 2003, Naples, Italy, November 19-21, pp. 1-15]).In this article we study GI from the topological point of view. Firstly, we explore the poset of compatible topologies on graphs and in particular on bipartite graphs. Then, from a graph we construct a particular compatible Alexandroff topological space said homeomorphic-equivalent to the graph. Conversely, from any Alexandroff topology we construct an isomorphic-equivalent graph on which the topology is compatible. Finally, using these constructions, we show that GI is polynomial-time equivalent to the topological homeomorphism problem (TopHomeo). Hence GI and TopHomeo are in the same class of complexity.",Compatible topologies on graphs: an application to graph isomorphism problem complexity
"This paper reports on a study in which observation, stimulated recall and semi structured interviews were used to report on children's knowledge and understanding of computer processes. It suggests a model for identifying stages in pupils' understanding of these processes with implications for how best to support and develop that understanding. This was a small scale, exploratory case study involving data collection in two schools. The study differentiated between: simple awareness of computer processes; immediate knowledge of how to use an item or carry out a process; ability to offer a simple explanation for a process; and ability to offer a more sophisticated explanation and to use knowledge to solve problems. It was found that all pupils had an awareness of basic input, output and storage devices and a reasonable level of confidence and competence in using the computer. However, children's knowledge tended to be confined to what they had acquired through experience. Interviews with the same pupils over a period of time showed little development in their conceptualisation of computer processes. Some examples of children's common misconceptions as well as partial and appropriate conceptions are given. An implication for the classroom is that more dialogue between teacher and pupil, or more realistically between teacher and groups of pupils, is needed so that the teacher can ascertain prior knowledge and understanding and present accessible explanations for pupils.",An investigation of children's conceptualisation of computers and how they work
"In this paper, we study cross-layer design for congestion control in multihop wireless networks. In previous work, we have developed an optimal cross-layer congestion control scheme that jointly computes both the rate allocation and the stabilizing schedule that controls the resources at the underlying layers. However, the scheduling component in this optimal cross-layer congestion control scheme has to solve a complex global optimization problem at each time, and is hence too computationally expensive for online implementation. In this paper, we study how the performance of cross-layer congestion control will be impacted if the network can only use an imperfect (and potentially distributed) scheduling component that is easier to implement. We study both the case when the number of users in the system is fixed and the case with dynamic arrivals and departures of the users, and we establish performance bounds of cross-layer congestion control with imperfect scheduling. Compared with a layered approach that does not design congestion control and scheduling together, our cross-layer approach has provably better performance bounds, and substantially outperforms the layered approach. The insights drawn from our analyzes also enable us to design a fully distributed cross-layer congestion control and scheduling algorithm for a restrictive interference model.",The impact of imperfect scheduling on cross-layer congestion control in wireless networks
"The lack of adequate training samples and the considerable variations observed in the available image collections due to aging, illumination and pose variations are the two key technical barriers that appearance-based face recognition solutions have to overcome. It is a well-documented fact that their performance deteriorates rapidly when the number of training samples is smaller than the dimensionality of the image space. This is especially true for face recognition applications where only one training sample per subject is available. In this paper, a recognition framework based on the concept of the so-called generic learning is introduced as an attempt to boost the performance of traditional appearance-based recognition solutions in the one training sample application scenario. Different from contemporary approaches, the proposed solution learns the intrinsic properties of the subjects to be recognized using a generic training database which consists of images from subjects other than those under consideration. Many state-of-the-art face recognition solutions can be readily integrated in the proposed framework. A novel multi-learner framework is also proposed to further boost recognition performance. Extensive experimentation reported in the paper suggests that the proposed framework provides a comprehensive solution and achieves lower error recognition rate when considered in the context of one training sample face recognition problem.",On solving the face recognition problem with one training sample per subject
"This work presents a modified version of the evolutionary structural optimization procedure for topology optimization of continuum structures subjected to self-weight forces. Here we present an extension of this procedure to deal with maximum stiffness topology optimization of structures when different combinations of body forces and fixed loads are applied. Body forces depend on the density distribution over the design domain. Therefore, the value and direction of the loading are coupled to the shape of the structure and they change as the material layout of the structure is modified in the course of the optimization process. It will be shown that the traditional calculation of the sensitivity number used in the ESO procedure does not lead to the optimum solution. Therefore, it is necessary to correct the computation of the element sensitivity numbers in order to achieve the optimum design. This paper proposes an original correction factor to compute the sensitivities and enhance the convergence of the algorithm. The procedure has been implemented into a general optimization software and tested in several numerical applications and benchmark examples to illustrate and validate the approach, and satisfactorily applied to the solution of 2D, 3D and shell structures, considering self-weight load conditions. Solutions obtained with this method compare favourably with the results derived using the SIMP interpolation scheme.",An efficient sensitivity computation strategy for the evolutionary structural optimization (ESO) of continuum structures subjected to self-weight loads
"Securing access to data in location-based services and mobile applications requires the definition of spatially aware access-control systems. Even if some approaches have already been proposed either in the context of geographic database systems or context-aware applications, a comprehensive framework, general and flexible enough to deal with spatial aspects in real mobile applications, is still missing. In this paper, we make one step toward this direction and present GEO-RBAC, an extension of the RBAC model enhanced with spatial-and location-based information. In GEORBAC, spatial entities are used to model objects, user positions, and geographically bounded roles. Roles are activated based on the position of the user. Besides a physical position, obtained from a given mobile terminal or a cellular phone, users are also assigned a logical and device-independent position, representing the feature (the road, the town, the region) in which they are located. To enhance flexibility and reusability, we also introduce the concept of role schema, specifying the name of the role, as well as the type of the role spatial boundary and the granularity of the logical position. We then extend GEO-RBAC to support hierarchies, modeling permission, user, and activation inheritance, and separation of duty constraints. The proposed classes of constraints extend the conventional ones to deal with different granularities (schema/instance level) and spatial information. We conclude the paper with an analysis of several properties concerning the resulting model.",GEO-RBAC: A spatially aware RBAC
"In this paper we describe the design of B-spline surface models by means of curves and tangency conditions. The intended application is the conceptual constraint-driven design of surfaces from hand-sketched curves. The solving of generalized curve surface constraints means to find the control points of the surface from one or several curves, incident on the surface, and possibly additional tangency and smoothness conditions. This is accomplished by solving large, and generally under-constrained, and badly conditioned linear systems of equations. For this class of linear systems, no unique solution exists and straight forward methods such as Gaussian elimination, QR-decomposition, or even blindly applied Singular Value Decomposition (SVD) will fail. We propose to use regularization approaches, based on the so-called L-curve. The L-curve, which can be seen as a numerical high frequency filter, helps to determine the regularization parameter such that a numerically stable solution is obtained. Additional smoothness conditions are defined for the surface to filter out aliasing artifacts, which are due to the discrete structure of the piece-wise polynomial structure of the B-spline surface. This leads to a constrained optimization problem, which is solved by Modified Truncated SVD: a L-curve based regularization algorithm which takes into account a user defined smoothing constraint.",Constraint-based design of B-spline surfaces from curves
"Pastiche scenarios draw on fiction as a resource to explore the interior 'felt-life' aspects of user experience and the complex social and cultural issues raised by technological innovations. This paper sets out an approach for their use, outlining techniques for the location of source material and presenting three case studies of pastiche scenario use. The first case study is an evaluation of the Apple iPod that explores the socio-cultural meanings of the technology. The second case study focuses on the participatory design of Net Neighbours, an online shopping system where volunteers shop as intermediaries for older people who do not have access to computers. The third is an in depth consideration of a conceptual design, the 'cambadge' a wearable lightweight web cam which, upon activation broadcasts to police or public websites intended to reduce older people's fear of crime. This design concept is explored in depth in pastiche scenarios of the Miss Marple stories, A Clockwork Orange and Nineteen Eighty-four that reflect on how the device might be experienced not only by users but also by those it is used against. It is argued that pastiche scenarios are a useful complementary method for designers to reason about user experience as well as the broad social and cultural impacts of new technologies.",Pastiche scenarios: Fiction as a resource for user centred design
"Model&dash;based supervision developed by systems analysts has become an acknowledged supervision aid, ensuring early detection of malfunctions and thereby allowing control of the availability and vulnerability of a process facility. However, it is associated with diagnostics of the process itself, and not of the process control situation, which is the veritable subject of supervision. The operator, facility, control triplet determines a complex situation that must be considered from multiple viewpoints beyond knowledge of the single behavioral model usually advocated in process control approaches. Representing different aspects of process control situation from multiple viewpoints notably allows the on line selection of the behavioral models relevant to the observed situation. Given the size of the application, it was essential not only to structure the knowledge required for the supervision system functions into operating system viewpoints, but also to provide a unique representation method for each viewpoint. The systemic approach SAGACE provides this formal representation framework and the methodology adopted to design and implement our industrial prototype relies on it. All these principles are illustrated by a description of an industrial application in the area of nuclear fuel reprocessing&colon; the size and complexity of the facilities and their high degree of computerization make reprocessing particularly well suited for supervision applications.",Supervision applied to nuclear fuel reprocessing
"In this paper, we present an image retrieval technique for specific objects based on salient regions. The salient regions we select are invariant to geometric and photometric variations. Those salient regions are detected based on low level features, and need to be classified into different types before they can be applied on further vision tasks. We first classify the selected regions into four types including blobs, edges and lines, textures, and texture boundaries, by using the correlations with the neigbouring regions. Then, some specific region types are chosen for further object retrieval applications. We observe that regions selected from images of the same object are more similar to each other than regions selected from images of different objects. Correlation is used as the similarity measure between regions selected from different images. Two images are considered to contain the same object, if some regions selected from the first image are highly correlated to some regions selected from the second image. Two data sets are employed for experiment: the first data set contains human face images of a number of different people and is used for testing the retrieval algorithm on distinguishing specific objects of the same category; and the second data set contains images of different objects and is used for testing the retrieval algorithm on distinguishing objects of different categories. The results show that our method is very effective on specific object retrieval.",Specific object retrieval based on salient regions
"The topic of cache performance has been well studied in recent years. Compiler optimizations exist and optimizations have been done for many problems. Much of this work has focused on dense linear algebra problems. At first glance, the Floyd--Warshall algorithm appears to fall into this category. In this paper, we begin by applying two standard cache-friendly optimizations to the Floyd--Warshall algorithm and show limited performance improvements. We then discuss the unidirectional space time representation (USTR). We show analytically that the USTR can be used to reduce the amount of processor-memory traffic by a factor of O(&sqrt;C), where C is the cache size, for a large class of algorithms. Since the USTR leads to a tiled implementation, we develop a tile size selection heuristic to intelligently narrow the search space for the tile size that minimizes total execution time. Using the USTR, we develop a cache-friendly implementation of the Floyd--Warshall algorithm. We show experimentally that this implementation minimizes the level-1 and level-2 cache misses and TLB misses and, therefore, exhibits the best overall performance. Using this implementation, we show a 2x improvement in performance over the best compiler optimized implementation on three different architectures. Finally, we show analytically that our implementation of the Floyd--Warshall algorithm is asymptotically optimal with respect to processor-memory traffic. We show experimental results for the Pentium III, Alpha, and MIPS R12000 machines using problem sizes between 1024 and 2048 vertices. We demonstrate improved cache performance using the Simplescalar simulator.",Cache-Friendly implementations of transitive closure
"This paper presents a family of bitmap algorithms that address the problem of counting the number of distinct header patterns (flows) seen on a high-speed link. Such counting can be used to detect DoS attacks and port scans and to solve measurement problems. Counting is especially hard when processing must be done within a packet arrival time (8 ns at OC-768 speeds) and, hence, may perform only a small number of accesses to limited, fast memory. A naive solution that maintains a hash table requires several megabytes because the number of flows can be above a million. By contrast, our new probabilistic algorithms use little memory and are fast. The reduction in memory is particularly important for applications that run multiple concurrent counting instances. For example, we replaced the port-scan detection component of the popular intrusion detection system Snort with one of our new algorithms. This reduced memory usage on a ten minute trace from 50 to 5.6 MB while maintaining a 99.77% probability of alarming on a scan within 6 s of when the large-memory algorithm would. The best known prior algorithm (probabilistic counting) takes four times more memory on port scan detection and eight times more on a measurement application. This is possible because our algorithms can be customized to take advantage of special features such as a large number of instances that have very small counts or prior knowledge of the likely range of the count.",Bitmap algorithms for counting active flows on high-speed links
